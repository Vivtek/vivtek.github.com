<html>
<head>
<title>Vivtek 2.0, or, The Blog</title>
<style>p, li, td {
font-family: arial, sans-serif;
}
p.smtitle {
margin-left:0px;background-color:#eeeeee;font-weight:bold;
}
.sidemenu p {
font-size:small;
margin-top:0px;
margin-bottom:2px;
margin-left:10px;
}
.title {
font-family: arial, sans-serif;
font-weight: bold;
font-size:x-large;
color:black;
background-color:#eeeeee;
}
.subtitle {
font-family: arial, sans-serif;
font-size:small;
}
.t2 {
font-family: arial, sans-serif;
font-weight: bold;
font-size:large;
color:black;
background-color:#eeeeee;
}
.st2 {
font-family: arial, sans-serif;
font-size:x-small;
}
.border {
border: 1px solid #336600;
}
.content {
}
pre.code {
  margin: .5em 1em;
  padding: 0.5em;
  border: 1px dashed #94bd8d;
  color: Black;
  background-color: #eff7ef;
  overflow: auto;
}
</style>
</head>
<body>
<table width="100%">
<tr>
<td valign="top" width="150px"><div class="sidebar">
<div class="border">
<div class="sidemenu">
<p class="smtitle">vivtek</p>
<p>[ <a href=/>home</a> ]</p>
<p>[ <a href=/blog/>blog</a> ]</p>
<p>[ <a href=/recent.html>recent</a> ]</p>
<p>[ <a href=/fiction/>fiction</a> ]</p>
<p>[ <a href=/toonbots/>toonbots</a> ]</p>
<p>[ <a href=/wftk/>workflow</a> ]</p>
<p>[ <a href=/projects/>programming</a> ]</p>
<p>[ <a href=/services.html>translation</a> ]</p>
<p>[ <a href=http://semantic-programming.blogspot.com/>semprog&nbsp;blog</a> ]</p>
<p>[ <a href=/contact.html>contact</a> ]</p>
</div>

<div class="sidemenu">
<p class="smtitle">blog</p>
<p>[ <a href="keywords.html">keywords</a> ]</p>
<p class="smtitle">blogger</p>
<p>[ <a href="http://semantic-programming.blogspot.com/">semprog</a> ]</p>
<p>[ <a href="http://startup-ideas.blogspot.com/">startups</a> ]</p>
<p>[ <a href="http://orgaprop.blogspot.hu/">politics</a> ]</p>
</div>

<hr />
<script type="text/javascript"><!--
google_ad_client = "pub-7508846022405297";
google_ad_width = 120;
google_ad_height = 600;
google_ad_format = "120x600_as";
google_ad_type = "text";
google_ad_channel = "";
//--></script>
<script type="text/javascript"
  src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
<a href="/blog/t/ah_google_adsense_we_hardly_knew_ye!.html">Where
did my Google go?</a>

</div>

</div>
</td>
<td valign="top"><div class="content">

<div class="title">Vivtek 2.0, or, The Blog</div>
<div class="st2"></div>
<hr>
<table>
<tr>
<td><div class="title"><a href="blogging_works_again.html">Blogging works again</a></div>
<div class="st2">2014-04-09 <a href="keyword_blogmeta.html">blogmeta</a></div><br>
I've finally got the blog publisher working, so all the historical posts and keyword assignments now
appear where they should (mostly; still have a couple of weirdnesses to figure out).
</p><p>
It all gets compiled and built on my local machine, written to static HTML, and pushed via git to Github,
which as you know, Bob, now serves my static content.  (This statement implies that my non-static content
is hosted elsewhere, which is technically true - it's still on my old box, but doesn't actually work right now.
That is a can of worms for another day.)
</p><p>
I had a lot of fun setting things up, and eventually I intend to post about the new site publication system.
But in the meantime, there are lots of other things I also want to write about, and my sabbatical week is
already half over just on the blog publishing system alone. (Sigh.)
</p><p>
Over the past two years, I've trained myself to use a note-taking system of my own design to track programming work
and ideas, and I've augmented that system to publish some notes to the blog. This is the first of those notes.
I hope to rebuild the habit of technical writing now that my translation productivity has risen - which should
at least potentially free up some time. Right?  (It took seven runs through this step to get it to work. Sigh.)
</p><p>
Anyway. There's lots to do. I'm going to get to it.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="house_update.html">House update</a></div>
<div class="st2">2009-07-31 <a href="keyword_house.html">house</a></div><br>
Reader tom writes in to say:
<blockquote>
<p>
Subject: Can anyone edit your blog?
</p><p>
I hope not. But just in case we can I'd just like to say (and I speak for all of
 humanity here) that you should let us people on the internet come and live in y
our carriage house whenever we feel like it. We wouldn't pay you per se. But thi
nk of it as wiki-living.
</p><p>
thank you,
-tom
</p>
</blockquote>
</p><p>
Tom, I can't tell you how horrifying an idea I find that, but the sad fact of the matter is that my family and I are living in the carriage house
at the moment, before getting underway renovating the big house.
</p><p>
However, finding tom's post in the spam filter (sorry, tom, nothing personal) reminds me that I kind of left my home-grown blog high and dry in favor
of the shiny new House-specific <a href="http://big-old-house.blogspot.com">Blogspot blog</a> I started shortly after arrival here in town.  Check it out; lots of status updates
and pictures.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="t_minus_a_month.html">T minus a month!</a></div>
<div class="st2">2009-04-14 <a href="keyword_house.html">house</a></div><br>
Just about one month until I get to see my house!
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="garfield_zen.html">Your Garfield zen for the day</a></div>
<div class="st2">2009-04-13 <a href="keyword_humor.html">humor</a></div><br>
Just a little something that occurred to me whilst perusing <a href="http://joshreads.com/?p=2637">the Curmudgeon</a>:
</p><p>
<img src="/images/garf.gif">
</p><p>
Sorry, sorry.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="lower_house_payments.html">Lower your house payments!</a></div>
<div class="st2">2009-03-31 <a href="keyword_house.html">house</a></div><br>
So I got this spam today: Lower house Payments 30%. And I realized: 70% of zero is still zero.
</p><p>
How cool is that?
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="riddle_me_this.html">So riddle me this</a></div>
<div class="st2">2009-03-24 <a href="keyword_economics.html">economics</a> <a href="keyword_house.html">house</a></div><br>
Tomorrow, March 25, I'm closing on the house.  The title company has the money, my sister has my power of attorney, and all is set; tomorrow I become a landowner again.  But there's something I don't understand, now that I've seen the HUD document detailing the financing of this deal.
</p><p>
John Fitch bought this house to form the Renaissance House in 2003, financing $54,000. I very much doubt he'd paid more than he had to. So five years later, I can't imagine he owed less than $50,000 or so. Following me?
</p><p>
Of the $8000 changing hands tomorrow, $2500 goes to the realtor, $700 to a property management company (coincidentally also the realtor, but that's not the point), $500 to a listing placement company, etc. etc. The actual current holder of the mortgage will be getting a tad over $4000 for this magnificent structure.
</p><p>
So here's my question. Given that this is more than a 90% loss -- why? Why would they foreclose? Is this one of those "pennies on the dollar mortgage purchases" one reads about? Who benefits?
</p><p>
Because somebody thought this was a good idea. The realtor clearly thinks so, with reason. I think so, because I get a cheap house (actually, Renaissance House may end up getting the use of the building for a year or two; we'll see). But the original holder of the mortgage?
</p><p>
Somebody's really insane in all this.  And it seems indicative of the whole economy.
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="latitude_40_0000.html">Latitude 40.0000!</a></div>
<div class="st2">2009-03-22 <a href="keyword_misc.html">misc</a></div><br>
So I was just measuring the latitude and longitude of my new house (closing in three days!) to play around with solar heating ideas -- when I noticed how close to 40 degrees of latitude it really is.  Just a shade further north, and it would be right smack on it.
</p><p>
So I panned north a little, then a little west...  My Dad's chicken house is <i>precisely</i> at 40.0000 degrees latitude.  My old bedroom is at 39.9997, if you were wondering.  The chicken house's longitude (and that of my bedroom) is 85.1561 degrees, not nearly as interesting a number.
</p><p>
Fascinating.  I hadn't really ever considered how small a distance one ten-thousandth of a degree of latitude might be, but it seems to be about ten or fifteen feet.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="comparative_prgg_modest_idea.html">Comparative programming - a modest proposal</a></div>
<div class="st2">2009-03-20 <a href="keyword_wftk.html">wftk</a> <a href="keyword_python.html">python</a> <a href="keyword_perl.html">perl</a> <a href="keyword_ruby.html">ruby</a></div><br>
So I had this really, really stupid idea a couple of days ago, but I just can't shake it.  See, I'm <a href="/wftk/perl_tutorial.html">rewriting the wftk</a> in Perl in tutorial form, something that I've planned for a really long time.
</p><p>
Well, here's the thing.  The Muse picked Perl, essentially because WWW::Modbot is an OOification of the original modbot stuff I wrote in Perl.  And the Term::Shell approach to the modbot turned out to resonate so well with what I wanted to do, that I just ... transitioned straight from the modbot back to wftk in the same framework.  But Perl -- even though I love Perl -- is not something I'm utterly wedded to, you know?
</p><p>
And now, I'm working in a unit-testing paradigm for the development.  I've carefully defined the API in each subsection, tested it, and know where I'm going.
</p><p>
So here's the stupid idea.  It just won't let go of me.  Why stick to Perl?
</p><p>
Why not take each class, each unit test, and <i>do that</i> in selected other languages?  It would be a fascinating look at comparative programming between the languages, wouldn't it?  And the whole point of the wftk is not to be restrictive when it comes to your existing infrastructure -- wouldn't one facet of that unrestrictiveness be an ability to run native in Python?  Ruby?  Java?  C?  Tcl?  LISP?
</p><p>
It just won't let go.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="renaissance_house.html">Renaissance House</a></div>
<div class="st2">2009-03-18 <a href="keyword_house.html">house</a></div><br>
The plot thickens.  It turns out that the edifice in question was until some point in time no earlier than August, 2007 an "intentional community ... balanc[ing] work, ministry, and restoration."  Here is a picture of a man with an alligator behind or to the side of the house:
</p><p>
<img src="/images/alligator.jpg">
</p><p>
Fascinating, the notion of buying a house with history attached.
</p><p>
The house was mentioned in <i>Quaker Life</i>'s <a href="http://www.fum.org/QL/issues/0309/index.htm">September, 2003 issue</a>.  "Renaissance House: A Ministry of Renovation".  It is still linked to from <a href="http://www.richmondquakers.com/">richmondquakers.com</a>, but the renaissancehouse.org domain is gone to the Bitbucket in the Sky, and no, the Wayback Machine only caches an older owner of the domain, not the 2005-2007-vintage site, which is a shame.  Pictures would have been nice.
</p><p>
<i>Update 2009-03-18</i>: The alligator's name is Amos Moses, and the man himself is John Fitch, the director of Renaissance House and former owner of the edifice in question.  It turns out he's a pretty nice guy (well, you expect that of Quakers) and the Renaissance House community is still going strong in two nearby houses with much lower payments.
</p><p>
So <i>that's</i> good.
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="Workflow_wftk_development_continues.html">Workflow wftk development continues</a></div>
<div class="st2">2009-03-17 <a href="keyword_wftk.html">wftk</a> <a href="keyword_test_driven_programming.html">test_driven_programming</a></div><br>
<a href="/wftk/perl_tutorial.html">A tutorial approach to workflow</a> -- right here!
</p><p>
So for the last month or so I've been working on rewriting the wftk from the ground up, in Perl, with proper object orientation (which is to say, given it's Perl, just enough object orientation to let me not trip over my feet, but not so much that it makes me crazy).
</p><p>
When I started my casual redevelopment, it was because I had discovered Term::Shell, which is <i>brilliant</i> for casual development of code where you're really not quite sure what you want to do with it.  Just start writing, and develop new commands as they occur to.  Cycle often.  This worked great for the current version of WWW:Modbot (available on CPAN, but not very well-documented yet, because I got sidetracked on the wftk, you see).
</p><p>
Anyway, I got quite a ways in before I realized I was starting to break things I'd done earlier.  So I did something I'd never done before, but had always intended to: I reorganized the entire project to use test-driven development.  Each new set of features or "featurelets" is in a subsection of a tutorial.  I write a new section of the tutorial, making up the code as I think it should work, copy that code into a new test section, and run "make test".  Then I fix it.
</p><p>
So <i>all of my development is fixing</i>.  I'm good at fixing.
</p><p>
You can see the current state of the Perl tutorial <a href="/wftk/perl_tutorial.html">over here</a>.  I've started with data manipulation as per my blog post last year about how the wftk <i>should</i> be structured.  It's going slowly because there's just so much functionality that needs to be in there -- but every day, I do a little subsection, and I feel a sense of accomplishment.
</p><p>
This is a sweet library.  You can define lists with a natural syntax, add data with a copy command or by throwing lists and hashes at it, then query it all with SQL.  It's everything I had always thought should be in the (data) repository manager, but didn't have the time to write, because in Perl, half of everything is already there and waiting for you on CPAN.
</p><p>
I'm going to start blogging each major point finished, just to continue to give myself a feeling of accomplishment, and also to provide a little bit of timeline for myself looking back.  This project has been running for about two months already; it'll probably take a year, at least, perhaps more -- so it's going to be fruitful to look back and see what I finished when.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="yiddish_policemans_union.html">The Yiddish Policeman's Union</a></div>
<div class="st2">2009-03-13</div><br>
If you haven't read Michael Chabon's <u>Yiddish Policeman's Union</u>, then please, I beg of you, sweetness, go out, purchase a copy of that book, and read it.
</p><p>
I think it may well be the best book I have read in a very long time indeed, and on many levels.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="achewood_ftw.html">Achewood FTW</a></div>
<div class="st2">2009-03-07 <a href="keyword_webcomics.html">webcomics</a> <a href="keyword_humor.html">humor</a></div><br>
<a href="http://m.assetbar.com/achewood/uuafT4CCF">Today:</a>
</p><p>
That the universe should misspend one mote of its grace and bounty on a fool like that is all the proof I need that the throne of the Lord sits empty.
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="adventures_in_plumbing.html">Adventures in plumbing</a></div>
<div class="st2">2008-09-21 <a href="keyword_life.html">life</a> <a href="keyword_plumbing.html">plumbing</a></div><br>
OK, so for reasons of decrepitude, the water shutoff for our place doesn't work (they weren't installed very well and our valve is snapped off entirely) so to do plumbing without local cutoff, like the showers, I have to turn off the water to our entire building (three apartments).
</p><p>
Anyway, the shower we don't use has been leaking since we moved in here, but recently it's been a lot worse.  I've been meaning to get around to it, but during the day the neighbors need water, and during the night, the kids sleep right next to it.
</p><p>
But the wife and kids are at Boy Scout camp tonight.  (Not me, I grew up in a log cabin, and that was all the camping I need for life.)  So after a grueling day of paperwork, at 2 AM I figured, well, why not now?  I've got the gasket set, etc.
</p><p>
So I shut off the water, go up, take a firm grip on the faucet with my pliers, and wrench it.
</p><p>
The whole thing came off in my hands.  As in, ripped off the pipe.
</p><p>
OK.  So, I have the water shut off to all the neighbors, it's 2 AM in Ponce and nothing is open (I mean nothing), and I don't have a car anyway.
</p><p>
Think, think, think.  This is not so different from shooting yourself in the foot with Unix sysadmin work, and I've done <i>that</i> often enough...
</p><p>
OK, so the inside of the faucet is this weird thing, with hot and cold in one pipe-looking thing, each feed with a, what, 3/8" or something copper tube.  I found the biggest screws I have on hand (remember: no car, no stores -- have to fix it with stuff I have in the house).  They're not big enough, but it's close.
</p><p>
Things I learned:
</p><p>
1. Electronics soldering irons are useless on plumbing.
</p><p>
2. There is a limit to how much teflon tape you can wrap around something and still make it work.  This limit appears to be a thickness smaller than the actual thread.
</p><p>
3. Despite our extensive collection of laboratory glassware, we don't have any rubber stoppers.  (This should be rectified.)
</p><p>
4. Although a stopper could easily be fashioned from duct tape, we don't have duct tape.  (This should definitely be rectified.)
</p><p>
4. Rule #47: if you don't even have duct tape, electrician's tape sometimes works.
</p><p>
5. Electrician's tape, rolled into a plug and jammed into the feed tubes, then screwed into place, reduces explosive leaks to drippy leaks.
</p><p>
6. Although a drippy leak at 3 AM is probably good enough for most people, I appear not to be most people.
</p><p>
7. Aquarium tubing, with electrician's tape wrapped around it to make a gasket, inserted into the copper tubing, then screwed in place with a 1/8" wood screw, doesn't even drip.
</p><p>
8. It is probably not quite balanced to blog about this instead of going to bed at 3:30 AM.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="trazan.html">Trazan, or, why it is good to clean out old bookmarks occasionally</a></div>
<div class="st2">2008-08-24 <a href="keyword_humor.html">humor</a></div><br>
So there I was, clicking down a list of old bookmarks I just ran across, tossing out the link rot and mulling over the stuff that's still in existence, and what should I find but this:
</p><p>
<a href="http://www.meehawl.com/Flash/Trazan/">Trazan</a>.
</p><p>
Man, I needed that.  I have to say -- I have great taste in bookmarks; they always seem to be stuff I like!
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="stormspam_fauxCNN_plus_virus.html">New botnet landing page, now with crunchy antiviral goodness!</a></div>
<div class="st2">2008-08-11 <a href="keyword_botnet.html">botnet</a></div><br>
If, like me, you've been wondering when the Storm people would do something new, your answer would have to be: an hour ago.
</p><p>
<a href="/projects/despammed/storm_page_analysis_4058864712-3087703986.html">Here's the analysis for the new page</a>, although I've just barely started.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="vista_cheating.html">And another thing about Vista!</a></div>
<div class="st2">2008-08-03 <a href="keyword_propaganda.html">propaganda</a></div><br>
Arghhh!  Have you seen these ads saying "How do people like Vista when they don't know it's Vista?"  Turns out they exposed a bunch of people to Vista and they (gasp) liked the user interface!  Wow!  So I should go out and buy Vista!
</p><p>
Uh .. no.  Vista has a great interface -- but that's not why people are failing to buy it in droves.  They're failing to buy it because it's new (and therefore buggy), it's broken by design (because it decrees that you don't own your PC -- license holders do, and if you're lucky, they'll let you use it), <i>and</i> because it represents money that I simply don't need to spend.
</p><p>
XP works for me.  I was slow to adopt even XP -- when my last Windows 95 PC couldn't keep up any more, I switched.  And frankly, the only reason I'm still running Windows is I'm a dinosaur who uses dinosaur software like Word and TRADOS (unfortunately, both absolute and utter kings of the translation industry -- the source of my money).
</p><p>
Anyway, if I can't vent on my blog, where can I?
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="storm_guys_lamer_today.html">Storm botnet standards slipping</a></div>
<div class="st2">2008-08-03 <a href="keyword_botnet.html">botnet</a> <a href="keyword_spam.html">spam</a></div><br>
Over the past couple of days as I datamined the <a href="http://www.despammed.com/">Despammed</a> spam archives for Storm botnet spam, I've grown to really enjoy their madcap subjects (latest <a href="/projects/despammed/stormspam.html">here</a>).  But today?
</p><p>
Guys!  "Obama bribing countrymen" or "McCain picks Osama bin Laden as VP" are hilarious!  But "Video News"?  "Top stories"?  Come on!  If you're going to hijack a million people's machines to spam us all, the least you can do is to continue to be entertaining about it.  This?  This is beneath you.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="botnet_spam_flood.html">My God, it's full of stars! Botnet spam flood</a></div>
<div class="st2">2008-08-01 <a href="keyword_spam.html">spam</a> <a href="keyword_botnet.html">botnet</a> <a href="keyword_javascript.html">javascript</a></div><br>
So I got halfway through analysis of my <a href="/projects/monkeywrench/cases/case_1/">first Javascript obfuscation discovered via spam</a>, when <a href="/projects/monkeywrench/cases/case_2/">another</a> came in, and then <a href="/projects/monkeywrench/cases/case_3/">another</a>!  And then I realized -- these were sent from botnet-controlled mailers that were slipping past my no-DSL filters at Despammed.  So how many were getting blocked?
</p><p>
Turns out, a lot.  Like, a <i>lot</i>.  So I'm going to have plenty of grist for this mill -- and the very fascinating thing is that it sure looks like there is a change in tactics each day.  So I'm going to try to go back through older instances and hope that people haven't fixed their servers yet for some, and I'm going to put up some early warnings to tell me about new ones -- but this is truly, truly fun.
</p><p>
Each of these mails has a faux news headline: "Michael Vick escapes from Federal jail", or "Beijing Olympics canceled", the one that first drew my attention.  Then the body of the mail has a <i>different</i> headline, and a link.
</p><p>
Turns out that different headline is drawn from the same list.  So I can check the Despammed.com spam archive (1.2 million spam emails on file at the moment) for other emails with that subject.  And so on.  This should allow me to build a database of subjects really, really easily.  And then I can simply scan for those subjects to find new instances.  If they select their headlines randomly (and I have no reason to believe they don't) this should allow me to find all their headlines and keep up with new ones at the same time.  Fun!
</p><p>
Once I've got that coded, I'll post a <a href="/projects/despammed/stormspam/">database page in real time</a>. [Updated to include link.]  That will be even more fun.  And <i>then</i> I can resume the de-obfuscation effort.  Actually, I've dusted off some old project idea notes and started work on the <a href="/projects/monkeywrench/">monkeywrench</a> to help me organize this stuff.
</p><p>
Note to anybody interested: the design philosophy of the monkeywrench is essentially a <a href="http://cogsci.indiana.edu">Hofstadter parallel terraced scan</a>.  But operated by a human (for now) in a <a href="/wftk/">workflow paradigm</a>.  I can sloooowly start to feel the various bits of my life coming together.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="cool_spammed_malware_trail.html">Cool spammed malware trail</a></div>
<div class="st2">2008-07-26 <a href="keyword_javascript_obfuscation.html">javascript_obfuscation</a> <a href="keyword_spam.html">spam</a> <a href="keyword_sleuthing.html">sleuthing</a></div><br>
I got a spam today saying the Beijing Olympics had been cancelled, so I was all "O hai, Botnet, I can has spamtrail?"  (Because I hear the Russians are using fake news headlines to induce people to open the mail now.  And part of this trail goes through Russia, as we'll see.)
</p><p>
The whole story (well, as much as I've followed and written down so far) <a href="/projects/despammed/bollettinogiuridicosanitario.html">is over here</a> because it is really detailed.  But it's fun so far, because not only is the main injection page obfuscated, it appears to be <i>encrypted</i> and the decryption code is itself obfuscated and located on a different server.  In Russia.
</p><p>
So far, it's been instructive, as always when one unravels these threads.  More later.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="msnbot_qbhp.html">MSNbot QBHP referrers</a></div>
<div class="st2">2008-07-20 <a href="keyword_traffic.html">traffic</a></div><br>
When I Wikiized the site, and started <a href="/wiki_index.html">indexing</a> the Wiki changes, I naturally also wanted to start looking at incoming traffic and referrers, as you can see on the "recent" page on the main menu.  And of course I then started refining it to suit my tastes.
</p><p>
I had already had a "preproc.pl" script to preprocess the logs and give me the hits I want to see.  That screens out spiders, everything I myself do from home, and (lately) any IP that posts spam to the forum or Wiki.  The remainder is proving pretty interesting.
</p><p>
Normally, one can filter out search engine spiders based on their agent.  But Microsoft, as always, follows their own rules (a little research on "search.live.com" and "QBHP" will show you plenty of griping.)  They use a normal IE agent string, but mark their search queries using the "form".
</p><p>
And you know, normally I wouldn't care.  But their search queries are <i>weird</i>.  They consist of a single word, usually (but not always) one found on the page, and if you're actually paying attention to search queries to determine what it is about your site people find interesting, <i>these won't help</i>.
</p><p>
So now my preproc script blocks everything from the 65.55.*.* block with "form=QBHP" in the referrer.  You just have to wonder what Microsoft is thinking, sometimes.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="anology_rule_of_law_capital_risk.html">Analogy - rule of law = capital risk</a></div>
<div class="st2">2008-07-19 <a href="keyword_politics.html">politics</a></div><br>
<a href="http://www.salon.com/opinion/greenwald/2008/07/19/law/index.html">Glenn Greenwald</a> writes today that "the idea that the Rule of Law is only for common people, but not for our political leaders and Washington elite, is pervasive among the political and pundit class, in both parties."
</p><p>
My immediate realization was that this attitude is exactly equivalent to the notion that financial risk applies only to individuals and to small business, but that large business (the "financial elite") is simply too large to be allowed to fail.  Oh, it's certainly allowed to <i>succeed</i>, mind you.  The banks in the mortgage market were happy enough to take home the profits from selling houses at rates far above their value.  But when their risky action proved, well, risky, they aren't expected to pay the price.  We do.
</p><p>
In a similar way, when our political elite breaks the law, they consider themselves to be too important to pay that price.  As a result, we all do.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="blogwiki_working.html">BlogWiki working</a></div>
<div class="st2">2008-07-16 <a href="keyword_wiki.html">wiki</a> <a href="keyword_blogwiki.html">blogwiki</a></div><br>
This is my first post using my Wiki Web interface with blog extensions.  There are still a few rough edges, but I believe this should work -- meaning I'll now be able to spout off on my blog at the drop of a hat.
</p><p>
Oh, sure.  I could have just installed MovableType like a normal person.  I am no normal person.
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="online_forensics.html">Online forensics</a></div>
<div class="st2">2008-07-16 <a href="keyword_spam.html">spam</a> <a href="keyword_internet_sleuthing.html">internet_sleuthing</a></div><br>
I've always had a soft spot for good explanations of Internet sleuthing for
fun and profit, and <a href="http://justinsomnia.org/2007/08/search-engine-marketeers-are-the-new-script-kiddies/">here's a dandy example</a>.
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="boingboing_d!.html">BoingBoing'd!</a></div>
<div class="st2">2008-06-08 <a href="keyword_fiction.html">fiction</a></div><br>
Well, yesterday was fun; I got BoingBoing'd for my <a href="/fiction/singularity_tales/">Tales of the Singularity</a>.  But today, I have regained my
cool sang froid about all that, and it's almost too much trouble to grep for boingboing.net referrers again.  (Almost.)
</p><p>
The only question is: having gotten a few eyeballs, how do you hold them?  I'm pretty sure the answer is: you don't.  They come, they go.  Having once
done something interesting, perhaps I can later continue to be interesting, but the key insight (besides hexapodia) is that you don't <i>keep</i>
eyeballs just because you want them.  That's putting the cart before the horse -- as <a href="/fiction/singularity_tales/tale_spambot.html">Paul Bunyan discovered</a>
<i>in my story</i>.
</p><p>
So <i>really</i>, my experience mirrored my story.  How ... meta.  And very unexpected.
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="wiki_tools_vivtek_style.html">Wiki tools, Vivtek style</a></div>
<div class="st2">2008-05-31 <a href="keyword_wiki.html">wiki</a> <a href="keyword_perl.html">perl</a></div><br>
So a week or two ago I suddenly was seized by the desire to Wiki-ize my
venerable old site.  I know, I know.  There are pages here I hand-coded in
1996.  There's stuff I tweaked into magnetic core memory using tweezers and
a small rare-earth magnet in 1948.  And we felt <i>lucky</i> to have that
cardboard box!
</p><p>
But, well, I love vi.  But lately, I've been feeling the need to stray from
my first love, and the ability to whack content into a simple form, click
a button, and have it published with no further ado, with all the sidebars
and stuff in place, well, I needed that.
</p><p>
<a href="/projects/wiki/">So I did it.</a>  And as everything else in my life, I did it with an idiosyncratic blend of Perl for the guts and AOLserver Tcl for the Web presentation and input parsing.  Eventually I will present the code.  But in the meantime, I'll note
two things.  First: it works, and works well, and works <i>extremely</i>
efficiently, because Wiki pages are published once when changed, and are then
available as flat HTML files when requested.  Contrast this with MediaWiki,
which hangs interminably on the database every damned time it generates the
sidebar menu.  Bad design, if you ask me (but of course, nobody did.)
</p><p>
Secondly: it integrates the beginnings of a pretty efficient data management
tool.  I'm using it for to-do lists right now, but I'm looking at various
other applications as well.  And it will probably feed right back into
workflow, if all goes well.  The most exciting thing about this aspect of
the system is that organized data can be anchored and commented upon in the
Wiki system.  I'll be putting this to much more extensive use in the analysis
of spam over at <a href="http://www.despammed.com">Despammed.com</a>, but even in the
context of my to-do list management it's proving a powerful tool for data
organization.
</p><p>
Other extensions I hope to explore are a CodeWiki (which will allow the
literate commentating of program code and other textual resources), a document
management tool for the management of binary objects like images, and, more
immediately, the replacement of this blog tool with Wiki-based code to do the
same thing.
</p><p>
This last month has been quite productive in terms of the code I use in my
everyday life, and the Wiki tool has been a big part of that.  So I hope this
burst of momentum continues.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="the_modbot_and_web-based_spam.html">The modbot and Web-based spam</a></div>
<div class="st2">2008-05-17 <a href="keyword_modbot.html">modbot</a></div><br>
As promised, a post on the modbot.
</p><p>
As you know, Bob, I first got into despamming more or less seriously in 1999, when I wrote <a href="http://www.despammed.com">Despammed.com</a> and foisted
it on an unsuspecting world.  And life intervened, as it often does, and Despammed's fortune has waxed and waned along with it, but I still retain
a fascination for the spam.
</p><p>
When XRumer hit the market in November 2006, and everybody suddenly started getting forum spam, I started work on the modbot, which is a set of
Perl code for the detection of spam on the Web.  And you know, there's lots of it.  In the context of various projects, I personally am responsible
for monitoring a <a href="http://www.donttasemeblog.com">MovableType blog</a> (two, actually), a <a href="http://www.nowarblog.org">Scoop installation</a>, a
<a href="http://mondoglobo.wftk.org/wiki/">MediaWiki site</a>, and the venerable old Toonbots forum on <a href="/toonbots/discuss.pl">WebBBS</a>.  And they <i>all get
spam</i>.  The type of spam changes over time.  The modus operandi changes over time.  And I find it all irresistible.
</p><p>
After my first iteration of the modbot, I got distracted for about a year, and all those venues started to accumulate spam, slowly but surely.
The Toonbots forum had some basic spam blocking in place, but it wasn't too effective, and of course MovableType has some fairly decent filters in place
and is moderated anyway, so spam didn't proliferate too badly there.  MediaWiki doesn't seem to be a real spam magnet, either (I suspect it needs a little
more savvy than the low-level help spammers hire can be expected to master.)  And so it was all pretty manageable until...
</p><p>
The nowarblog.org Scoop installation (which I'd nearly forgotten about) was slowly growing in its server demand.  I hadn't realized it for a while, because
I had just assumed it was MediaWiki being the hog it most certainly is.  I recognized that eventually I'd have to track it down, but I'd been quite busy.
But finally, things got so bad I couldn't neglect it any more -- Apache was spending so much time locking the CPU that sendmail wasn't actually getting
me my mail, and that was a problem for the paying work.
</p><p>
So, groaning at the notion I was going to have to get into MediaWiki's PHP and cache it or something, I took a closer look.  And it turned out that
while I wasn't watching, the nowarblog.org Scoop installation had collected 34,000 comments and change.  Shyeeah, like <i>that</i> was gonna happen.  It
was spam.  Scoop doesn't react well to large numbers of comments -- each hit to a spammed page (including every new spam comment post) was hanging on the
CPU for over a minute.  Of course I knew: that meant war.
</p><p>
I dusted off the modbot code, because I wanted to archive the spam properly because <i>eventually</i> I'm going to do some analysis.  And two days later,
there were only twenty comments left on nowarblog.org (not for lack of trying, of course.  The modbot just cleaned up 1400 spam comments this afternoon.)
Next I adapted it to the Toonbots forum; that went well, too.  The modbot is carefully written to be as modular as possible, because spam crops up all
over the place and I want one single way to filter it all.
</p><p>
My next target is MovableType, which has two categories of spam with different characteristics.  There's normal comment spam, and I have a few
techniques which will work well for that.  But the other category is trickier, and blog-specific: trackback spam.  Donttasemeblog.com gets about five
trackback spams a day, and I'm still not entirely sure how to block them.  Ultimately, one test is going to be to check the link being spammed; for
trackbacks, if it forwards to another site, I regard that as spam.  Haven't implemented it yet, though.
</p><p>
MediaWiki spam is going to be tougher still; I'm going to need to write code to back the revisions out carefully, and I'm not yet sure how that's
going to work without shooting myself in the foot.  The really pernicious feature of MW spam, though, is that the spammers typically deface existing
content.  That's really not good.  So it's going to be necessary.
</p><p>
One mode of the modbot is going to have to be email-based.  For simple Web-post forms which deal with email, I want to be able to filter that spam before
it comes to me.  The normal email filters at Despammed, of course, can't begin to deal with that, because as email it's entirely legitimate.  Instead,
a judgement has to be made based on its actual content.  That's on the to-do list, too.
</p><p>
Ultimately, it will be impossible to block spam -- there's no way for a machine to know with absolute certainty who you want to hear from.  But that's
exactly what makes it so very fascinating.  The vast majority of spam is obvious, but sometimes ... sometimes you have to think about it.  And the
natural response of spammers will have to be to get better at spamming.  I truly believe that the spam arms race is where natural computer intelligence
has a good chance of arising.  So ... I despam.  It's my way of immanentizing the Eschaton.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="generalized_document_management_framework.html">Generalized document management framework</a></div>
<div class="st2">2008-05-12 <a href="keyword_document-management,.html">document-management,</a> <a href="keyword_wftk.html">wftk</a></div><br>
Something that a lot of my project ideas have in common lately is a kind of generalized document management framework.
</p><p>
This isn't as impressive as it sounds, actually.  But it's kind of a key notion for Web 2.0 stuff -- if you want collaboration, you have to
have a place to store that collaborated content.  That place is the document management system.
</p><p>
Let's consider this for a moment, in the context of the <a href="/fantasy_namer/">fantasy name generator</a> from last week.  That fascinating little
thing takes a simple document -- the language definition -- and runs a Perl script on it, yielding some interesting results.  The
<a href="/toonbots/toon-o-matic/">Toon-o-Matic</a> does the same thing; it takes a simple XML document and runs a whole sheaf of Perl on it to generate an image.
A Wiki for my general site content, or a forum, or even a simple Web form post, can all be seen as doing the same thing.  An online programming tool;
same thing.  All these systems share a component -- the user can submit a large (ish) text object, often based on an existing one, for arbitrary processing,
which usually has some visible effect on the system.
</p><p>
If you just look at that little unit of functionality, you can imagine lots of attractive ways to extend it, too.  As I mentioned in my initial post
on the fantasy namer, you can suddenly imagine allowing people to name a particular definition.  You can imagine a page devoted to it, perhaps including
all the results it's generated -- maybe in ranked order.  All that's a lot of different features, but the central one is simply being able to store
and manipulate that central document.  It provides a hook on which you can start hanging interaction; without the hook you can't even conceive of
where to start.
</p><p>
So this notion's been in my head lately.  Oh, I'm sure all this was done in much more diligent detail ten years ago.  (Well.  Seven or eight years ago,
anyway.)  In fact, I can think of a couple of systems -- but they're all too damned complicated.  What I'm after is the ability to boil these things
down to their essence, to provide a language of thought about these systems.  For myself, anyway.  Assuming you exist, you may or may not benefit.
(But I'll bet you would.)
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="fantasy_name_generator.html">Fantasy Name Generator</a></div>
<div class="st2">2008-05-03 <a href="keyword_generative_linguistics.html">generative_linguistics</a> <a href="keyword_name_generator.html">name_generator</a></div><br>
Man, just when I think it can't possibly get any busier in my life, well, it does.  So I just haven't had much time left
over to do programming, and that makes me sad.  But when this happens, eventually the Muse forces me to code something.
This week, besides getting back to the modbot forum despammer (on which topic I will write later), suddenly last night
I found myself writing a Perl script to generate fantasy country names based on a program written in some antediluvian
BASIC dialect in the 70's by <a href="http://papersky.livejournal.com/387406.html">Jo Walton</a>.
</p><p>
Once I'd written it, I realized it would be cooler if it were online -- because <i>everything is</i>.  And so I wrote a little
Tcl wrapper based on my Google count wrapper, and <a href="http://www.vivtek.com/fantasy_namer/">slapped it up.</a>  And played with it a lot.
</p><p>
So now I have a lot more ideas, of course.  I wrote it to generate a random number of syllables up to a maximum, then gave it
syllable forms like "cvv", "vv" (for consonant-vowel combinations).  The vowels and consonants are likewise simple lists.
You can put accents on the vowels, optionally.  You can specify prefixes or suffixes.  You can make letter choices for the consonants
according to some scheme Jo dreamed up.
</p><p>
But <i>really</i> what this is -- or could be -- is a generic generative grammar at the phonological level.  So I could well imagine
making it more general, by providing a nested and arbitrary hierarchy of phonemes (liquids, fricatives, front and back) and providing more
complicated options for rules.
</p><p>
And on the wrapper end, wouldn't it be nice to be able to save a language spec, bookmark it, name it, commment on it, mail it to your
friends, etc?  How about an "evolutionary strategy" to search for types of words you like the best?  Heck, there's all kinds of stuff that
could be done.
</p><p>
But this was fun.  And from the traffic it's getting, I'm not the only one who finds it hypnotic.
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="mediawiki_double-plus_good.html">MediaWiki double-plus good</a></div>
<div class="st2">2007-12-03 <a href="keyword_mediawiki.html">mediawiki</a></div><br>
I have seen the future, and it calls itself MediaWiki.
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="despamming_vivtek.html">Despamming Vivtek</a></div>
<div class="st2">2007-10-26 <a href="keyword_despammed.html">despammed</a></div><br>
I finally took the plunge, after 8 years.  All vivtek.com mail is now going through the Despammed filters.  I just couldn't take the sheer mass
of spam any more.  (I get more than 1000 email spam messages a day and yes, for 8 years I've sorted through them by hand.  Did I mention I read
quickly?  But lately I've been letting the spam slush pile get too big.  Time for a change.)
</p><p>
There are some issues I still want to address -- like setting up a whitelist for customers to be on the very safe side -- but I have to say that
the drop in spam in my actual Inbox has been (1) incredible and (2) so worth it.
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="towards_wftk_2_0.html">Towards wftk 2.0</a></div>
<div class="st2">2007-10-24 <a href="keyword_workflow.html">workflow</a> <a href="keyword_wftk.html">wftk</a></div><br>
I never really officially released wftk 1.0, of course (the magnitude of
the task simply grew and grew and I became less and less certain of my
approach -- and then the recession happened.)  But I've been thinking a lot
of a more reasoned approach lately, and maybe it's time to reboot the wftk
project and start more or less "from scratch".
</p><p>
I see the modules in this new approach more or less as the following:
</p><p>
<ol>
<li>Data management<br>
This is the basic list-and-record aspect that the repository manager
started out addressing.  Now, of course, there is SQLite.  So a principled
workflow toolkit would start by using SQLite for local tables, and add
"external tables" (for which the new SQLite has an API) defined in what
SAP now calls the "system landscape".  It's amazing, by the way, how much
of my thinking over the past few years I see reflected in what SAP is doing
lately in their NetWeaver stuff.
</p><p>
<li>Document management<br>
Document management, as I see it, consists of: (1) actual central storage
and versioning of unstructured data; (2) storage of metadata about documents;
(3) parsing and indexing of unstructured data to produce structured data
elsewhere in the system.  The document manager should be able to work well
in either situations where it controls storage (and thus can initiate action
whenever anything is changed) or when it merely indexes a storage which can
be changed externally -- that latter might be, for instance, management of
a Website's files in the file system.  Or just your system files on a Windows
machine.  Periodically, the document manager could check in and see whether
things had been changed, and if so, trigger arbitrary action.
</p><p>
<li>"Action" management<br>
A central script and code repository defines the actions that can be taken
by a system.  I consider this to include versioning and some kind of
change management and documentation system, including literate programming
and indexing of the code snippets.  The build process should also be managed
here, and should be capable, for instance, of taking algorithms written in
C, compiling them into DLLs or .so dynamic load libraries, and calling them
from Perl, say.  Ultimately.
</p><p>
Actions, documents, and data would have a nested structure, by the way; there
would be global actions, application actions (a given case or project could
be an instance of an application), and project/instance actions, and the same
applies to data and documents, perhaps.  Originally I'd thought of doing the
same for users or organizational units, but I really think that if you're
defining a common language of actions and data, it should be organized into
applications and, perhaps, subapplications or something.  But <i>not</i>
differ by user!  (I might be wrong, of course.)
</p><p>
The above three modules together allow a data-flow-oriented processing
system, but we're still
missing:
</p><p>
<li>Outgoing interfaces<br>
This includes publishing of HTML pages, outgoing mail notifications, other
notifications such as SMS or ... whatever.  Logged, all of it.  It includes
report generation into the document management system or the file system,
generation of PDFs, etc.
</p><p>
<li>Incoming interfaces<br>
Given the parsing power of the document management module, this is more an
organizational module.  The system should be able to receive email, parse
it, and take action.  Conversational interfaces are covered here as well,
from SMTP- and IMAP-like state machines to chatbot NLP interfaces.  And
of course form submission from Websites also falls into this bucket.
</p><p>
<li>Scheduling<br>
Whether running on Unix with cron and at, or Windows with ... whatever the
hell Windows offers, the system should have a single unified way of dealing
with time in a list of scheduled tasks.
</p><p>
<li>Users, groups, roles, and permissions<br>
This module would be in charge of keeping track of who is performing a
given action and whether they're allowed to do so.  The original wftk already
provided a really nice mechanism which would still be nice here: when judging
permissions, any action can get the answers "yes, it's allowed", "no, it's 
not allowed," and "it's allowed subject to approval."  That last invokes
workflow for <i>any arbitrary action</i> and that would be a powerful
abstraction for nearly any system.  It's essentially transaction management
on a much more abstract scale.
</p><p>
And finally, the icing on the cake,
</p><p>
<li>Workflow<br>
The two components which make workflow workflow are a task list (tasks are
hierarchical in nature and so a task can have subtasks as a separate project)
and a workflow process definition language.  The new wftk should be able to
work with any workflow formalism -- after all, the process definitions are
considered scripts in the versioned script document repository.  The existing
wftk engine will almost certainly fit in here with little modification.
</p><p>
The primary benefit of workflow is that it allows dissociation over time.
A running workflow process isn't active on the machine for the weeks or months
it might require -- it's simply a construct in the database that gets
resurrected as required.  There are a boatload of applications in general
programming, but nobody sees them as workflow because everybody "knows"
workflow is a business application.  The wftk was to have changed that, and
I think the potential's still there.
</p><p>
There's also a case to be made for a module for
</p><p>
<li>Knowledge management<br>
This portion of my thinking is a little less organized.  I'd kind of like
to lump some kind of concept database in here, perhaps a semantic parser
or something.  Originally I'd thought that AI would go in here, but I
actually think that Prolog might just be another action script language.
This is definitely a blurry line in its native habitat, and crikey, he's 
not happy to see me here!
</p><p>
But the point of a blog is to write this stuff down as it occurs. So there
you have it, this would sit on top of the workflow.  Think of it as a way
to build smart agents into your data/document/action/workflow management
system.
</ol>
</p><p>
And there you have it -- my plan to wrap up the thought and work of eight years.
Oh, and this time I'm not bothering with licensing requirements.  Like SQLite,
wftk 2.0 will be in the public domain.  I don't really care if I get credit
or not for every little thing, because frankly, anybody who counts will figure
it out.  And have you noticed how <i>everything</i> these days uses SQLite?
It's because -- well, primarily because it works, but also because you don't
have to worry about legal repercussions of using the code.
</p><p>
That's where wftk document management should be, where wftk workflow should
be.  Simple, easy to use, and ubiquitous.
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="chatbot_framework.html">Chatbot framework</a></div>
<div class="st2">2007-10-07 <a href="keyword_chatbot.html">chatbot</a> <a href="keyword_nlp.html">nlp</a></div><br>
Something I've wanted to do for a couple of years now is a sort of online
chatbot framework thing.  In other words, this would be a testbed for different
language analysis techniques that could be played with online and tested
against real people.
</p><p>
An extension would be to connect a given chatbot to some other chatbot out
there somewhere, and see them talk to each other.  That could be fun.
</p><p>
The basic framework for this kind of venture could be pretty simple, but
could, of course, end up arbitrarily complex.  You'd need some kind of 
principled semantic framework (which would start at a simple box with words
in it and ramify through increasingly sophisticated syntactic and semantic
analyses -- the idea is to have a framework which can support both simplistic
single-word pattern matching to select a response, or use sentence frames
to extract some subject patterns to be manipulated in the response, right
up through a hypothetical Turing-complete NLP parser.)
</p><p>
The session would contain a list of facts and "stuff" which corresponds to the,
I dunno, dialog memory of a conversation.  There could optionally be some kind
of database memory of earlier conversations with a given contact.  Again, this
would run the gamut from simple named strings to be substituted into a response
pattern, to complete semantic structures of unknown nature, which would be
used to generate more sophisticated conversation.
</p><p>
Then the third and final component would be the definition of a chatbot itself.
This would consist of a set of responses to given situations (a situation being the current string incoming plus whatever semantic structure has been
accumulated during the course of the conversation.)  There could be a
spontaneity "response", i.e. something new said after some period of time
without an answer from the other.  Again -- it should be possible to start
small and stupid, with simple word patterns, random-response lists, and the
like, and build upwards to more complicated semantics.
</p><p>
The ability to detect and switch languages would be of great use, of course,
and there should be some kind of facility for that as well.
</p><p>
Wouldn't it be nice to be able to build a chatbot for language practice in,
say, Klingon or L&aacute;adan?  I mean, how else could you reasonably practice
a constructed language?
</p><p>
Anyway, when I have time, I'll certainly be doing something with this idea.  Any year now, yessir, any year.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="terrorism_detectors.html">Terrorism detectors</a></div>
<div class="st2">2007-09-27 <a href="keyword_text-analysis.html">text-analysis</a> <a href="keyword_politics.html">politics</a></div><br>
This week, for the second week in a row, <a href="http://www.boingboing.net/2007/09/26/do-you-blog-like-a-t.html">Boing Boing</a> features a University of Arizona
initiative to "identify people online by their writing style".  Homeland Security is of course all over this whiz-bang tech like ants on honey, because...
well, I started a comment on the program, only to realize this would better be a blog post.
</p><p>
1. I'll accept that it's possible to come up with some "similarity metric" that says "A is 99% similar to A' but only 32% similar to B", in the sense that we have such-and-such a probability that a given text was written by a certain person.  (So we end up with "similarity islands" of texts in the metric space, and we call each of those islands a writer.)
</p><p>
But that means that for any text we have only a certain (finite and non-certainty) probability that a given text is actually written by A.  So let's get entirely wild and assume some government researcher with more money than brains, working alone in a highly technical and difficult field, somehow writes an algorithm as good as, say, Google's algorithm for determining the <i>topic</i> of a page, which is inherently an easier topic.
</p><p>
The result?  We will be able to find terrorists online as well as Google can avoid giving us crap search results.  And forgive me for saying this, but nobody in their right minds would arrest someone based on a Google result.
</p><p>
OK, #2.  We are categorizing writers <i>and potentially calling them enemies of the state</i> based on WHAT THEY WRITE.  Now, I know that not all the readers of this blog are Americans, but here in America, we have something called the Constitution which means that it is not a crime to write things.
</p><p>
Ah, hell, all snark aside, even if this works, it's still misguided for patently obvious Constitutional reasons.  And it's not going to work, not the way they think, because --
</p><p>
3.  What this all boils down to is this.  Politicians and technocrats think that the world is divided into two groups of people: "our" people, who do what we say and pay us taxes so we can buy nice houses in Virginia, and "those" people, who rouse the rabble and put our salaries in jeopardy.  "Those" people, this year, we call "terrorists".  Earlier they were "communists", or "labor organizers", or "civil rights activists", or whatever -- the main thing to remember is that everything is stable unless troublemakers stir things up.
</p><p>
And we used to be able to know who those people were, because they looked funny.  But on the Internet, nobody knows you're a dog -- and so there is a perceived need, whether it's possible or not, for a technology to identify dogs.  Or "terrorists" -- but what they <i>really</i> want is to be able to draw a line down the middle of the Internet between safe people and troublemakers.
</p><p>
And what <i>that</i> means is that the freewheeling exchange of ideas -- OK, and 90% crap -- which is the Internet?  It's gone sufficiently mainstream that these people regard it as a threat, exactly like certain neighborhoods, or certain movements, have been in the past.  It's <i>too free</i> for comfort, and it's <i>too well-known</i> to be ignored.
</p><p>
I don't know how it'll play out.  But this story really exposes a seamy underside of our society.  It's depressing.
</p><p>
And then there is the notion of spoofing such an algorithm, as researched by none other than Microsoft, at
<a href="http://research.microsoft.com/research/pubs/view.aspx?id=1623&type=Publication">Obfuscating Document Stylometry to Preserve Author Anonymity</a>. 
This may well be illegal research at some point in a dystopian future...
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="9_11_a_little_late.html">9/11, a little late</a></div>
<div class="st2">2007-09-12 <a href="keyword_politics.html">politics</a></div><br>
I just found myself posting this in comments on another site, and it
expresses my feelings pretty damned well.  This is a technical blog,
but it's my blog, and in this one instance, I feel justified about a
political post.  And it's not like anybody reads this anyway -- so
assuming you even exist, you have no reason to complain.
</p><p>
Thus runs the missive:
</p><p>
Bah.  I don't like to rehash 9/11 because the second thing I thought, walking
into the IU Union and seeing the smoking tower on the TV, was "Reichstag",
and in this one thing, Mr. Bush has not failed me.
</p><p>
The first thing was -- holy shit THERE's something you don't see every day.
</p><p>
You know what?  The actual specific damage there was nothing to the United States.
Compare it to the damage Hitler did in Europe, then come back and tell me it
was at all significant.  The only damage it did was in America's collective
head -- it was a bee sting, and the last six years have been anaphylaxis.
Not sure yet whether it was fatal shock, but the patient still doesn't look good.
</p><p>
Now these same people, after 9/11-ing for years to justify stupidity and
blood in Iraq, telling us a tinpot third-world embargoed communist was
as dangerous or more dangerous to our mighty nation than the heavily
industrialized Germany we faced down and beat while fighting on another
front entirely -- against <i>ninjas</i> for cryin out loud -- those people
are now telling us we're in an existential fight with Iran over the
fate of Western Civilization.  They've been at war with us for thirty years,
and it's been an existential threat, but we just ... haven't noticed?
Does anybody but me see how fricking stupid that sounds?
</p><p>
I spent a lot of time and energy being liberal and anti-war between 2001
and 2004-ish, killed my business thinking about politics instead of
noticing the recession, ran up a shitload of unpaid back taxes and debt
while killing my business, and every day I watched a pack of deadbeats
getting richer and richer off America's pain, all because of America's
Reichstag and the willingness of the American people to treat international
politics like a horror movie.  And that fucking monkey, excuse my freedom,
can't even wipe the smirk off his face, even now.
</p><p>
I just don't care any more.  America will recover from its panic attack,
or it won't.  It doesn't matter what I do or say.
And that's what 9/11 means to me.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="work_on_the_toon-o-matic.html">Work on the Toon-o-Matic</a></div>
<div class="st2">2007-08-25 <a href="keyword_toon-o-matic.html">toon-o-matic</a> <a href="keyword_toonbots.html">toonbots</a> <a href="keyword_sisyphygean-tasks.html">sisyphygean-tasks</a></div><br>
So I actually did something with the <a href="/toonbots/toon20070215.html">Toon-o-Matic</a> for a change.
It was fun!  Didn't finish what I wanted to do, but at least there was an actual new kind-of-episode
for the first time since 2006.
</p><p>
Maybe next year I can do another.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="ocr_in_python.html">OCR in Python</a></div>
<div class="st2">2007-08-16 <a href="keyword_tower_defense.html">tower_defense</a> <a href="keyword_evolutionary_programming.html">evolutionary_programming</a> <a href="keyword_ocr.html">ocr</a> <a href="keyword_python.html">python</a> <a href="keyword_pil.html">pil</a></div><br>
OCR in Python.  There isn't any, to speak of.  While there do exist a few
open-source OCR projects (Conjecture seems to have a great deal of promise!),
none of them play well with Python.  I may want to rectify that at some point.
</p><p>
Anyway, growing bored of simply writing AutoHotKey scripts to play Tower
Defense, I quickly realized that I really needed a tool to start the game
for me, and track the score and other stats for later analysis.
</p><p>
The first part was no big deal; I whipped out a PyPop applet that could launch
a URL.  Since I wanted a window that was sized according to the Flash object,
that required putting together a local HTML file that could run some JavaScript
to pop up the window I wanted, then close itself afterwards.  I'll document
that <i>when I have time</i> (I propose the abbreviation wIht to save my time
typing that phrase.)
</p><p>
Well.  That was fun, and it worked, but I really wanted something to monitor
the score for me, and timing, and ... stuff.  Which meant that I would have
to read the actual graphical screen, because there's no handy-dandy textual
output on that Flash app.
</p><p>
You'd think that would be trivial in 2007.  But you'd be wrong.
</p><p>
Getting a snapshot of the screen was easy enough.  I wrapped win32gui to get
a window handle by the title (I'll document this later: Idtl), then installed
PIL to grab the actual graphical data and manipulate it.  To warm up with that,
I set a timer to grab a four-pixel chunk of the screen so I could see whether
the Flash had started or not (Tower Defense goes through an ad screen, then
a splash screen, and only then does the game start.)  That took a little
putzing around, but the result was gratifying: my little utility could tell me
when the game was ready to play.  As long as the window was on my primary
monitor, anyway (turns out PIL is not good with multiple monitors -- who knew?)
So it turned out I had to move the window before all that.
</p><p>
And then I could grab the sections of the screen with the numbers on them
for score, bonus, lives, timer, and money... And then it all screeched to a
halt, because there are no open-source Python OCR libraries.  At all.  And
clearly I don't have the time to adapt something -- hell, I don't even have
time to do all this.  I don't even have time to <i>write this blog entry</i>.
</p><p>
So of course I did the natural thing.  I wrote my own special-purpose OCR,
because clearly that would be saving time.  I saved four hours before I started
falling asleep, and it still can't tell 8 from 0 (but it does a fine job on the
rest of the digits.)  It was a lot of fun, actually.  Idtl.
</p><p>
So.  Proposal.  It would be nice to work some with Conjecture, and produce
the following: (1) a Python binding that can work in memory with PIL bitmaps,
(2) a Web submitter for test graphics, and (3) an online tester and test
database reporter.  That would be really cool.  Of course, only wIht.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="tower_defense_automation.html">Tower Defense automation</a></div>
<div class="st2">2007-08-10 <a href="keyword_evolutionary_programming.html">evolutionary_programming</a> <a href="keyword_tower_defense.html">tower_defense</a></div><br>
So it turns out that AutoHotKey can indeed play Tower Defense; I recorded a
script to set up my favorite opening last night.  It occurs to me that it
would be quite easy to set up a simple "strategy-description language" which
would be a series of commands to be executed in order.  Translating that
higher-level language into an AHK script would be trivial.
</p><p>
So at least it would be possible to track strategies and compare them.  That's
step one in an evolutionary approach.
</p><p>
Couple that with a Python front-end to make it all more convenient, and
we're off to the races!
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="tower_defense_game_notion.html">Tower defense game notion</a></div>
<div class="st2">2007-08-05 <a href="keyword_tower_defense.html">tower_defense</a> <a href="keyword_evolutionary_programming.html">evolutionary_programming</a> <a href="keyword_project.html">project</a></div><br>
So I have gotten sucked into this cool game,
<a href="http://www.handdrawngames.com/DesktopTD/game.asp">Tower Defense</a> -- apparently
there's a whole genre, but this is the one I've been playing with the past
couple of days.
</p><p>
It's a Flash game, and the premise is simple.  You have a grid with two
incoming gates and two outgoing gates.  Enemies come in the incoming gates,
and you have to kill them before they get to the outgoing gates.  You lose
one health point for every enemty that gets through.  You have 20 health
points and no way to get more.
</p><p>
So -- the point of the game is to build defense towers which run automatically.
Each enemy you kill nets you a small amount of money, which you can use to
build new towers, or upgrade existing ones.  (You need to upgrade to keep up
with the steadily increasing toughness of the enemies, of course.)
</p><p>
It's fun.  I thought of a fun way to do some really cool programming with it.
AutoHotKey can find the Flash window, and can scan the screen for whatever
you need.  It can click things, too.  So  my notion is to write an AHK
front-end to play the game, with an IP connection to a strategy server.  The
strategy server would then use evolutionary programming to come up with 
board arrangements, and over time (perhaps a great deal of time, who knows?)
you could watch the strategizer get <i>really good</i> at Tower Defense.
</p><p>
Or not.
</p><p>
But it would be fun to try it.
</p><p>
My notion is that the instructions for a given strategy would consist of a
series of builds at given coordinates, intermixed with upgrades to the towers
built.  Each instruction would only execute after there was enough money
available for it.  So you'd have a string of instructions which would express
a given board setup.
</p><p>
To combine strategies, you'd just ... well, I guess you'd just cross the
streams, so to speak, taking two (or more?) successful strategies, cut them
at a random spot, and splice them together.  You might want to have some
cleanup rules (you can't upgrade squirter #3 if you didn't build three
squirters, for instance.)
</p><p>
And then you'd let it run for a few weeks.  You could get other people to
download your script and run strategies, too.
</p><p>
It could be very, very cool...
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="allofmp3_com_dead.html">AllOfMP3.com dead?</a></div>
<div class="st2">2007-07-02 <a href="keyword_allofmp3.com.html">allofmp3.com</a> <a href="keyword_your-rights-online.html">your-rights-online</a></div><br>
At least since last Wednesday night, AllOfMP3.com has been dead in the water.
Given that Russia promised the United States to "make it illegal" for 
AllOfMP3.com to continue operations by June 1 of this year, I'm wondering
if that's the situation.
</p><p>
Possibly it's technical issues (they've been known to happen in the past)
or possibly somebody has been too pre-emptive in their removal, but still --
it's been five days now.  And nobody in the world seems to have noticed.
Except me.  World exclusive?  Am I the next tech-Drudge?  Only time will
tell.
</p><p>
BREAKING NEWS!  COVERAGE UNFOLDING!
</p><p>
Anyway, I even got uber-paranoid and tried traceroutes from other countries
than the United States (hey, it <i>could</i> have been the case!), but the
IP is dead from Austria and even Byelorus.  I figured Byelorus is close enough
that there would be no profit in trying the rest of the alphabet.
</p><p>
Watch this space for further details.  Assuming I discover any.  And remember
poor AllOfMP3.com in your prayers.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="catching_up!.html">Catching up!</a></div>
<div class="st2">2007-06-27 <a href="keyword_blogmeta.html">blogmeta</a> <a href="keyword_biofuel.html">biofuel</a> <a href="keyword_aquaponics.html">aquaponics</a></div><br>
I'm not dead!  Just really busy.
</p><p>
So I've been wanting to put up some information about organic produce,
the digestive system, bioplastics, and so on, but (as usual) don't have
the time.  And then today, xkcd comes up with <a href="http://xkcd.com/c282.html">this</a>.
Ha!  Definitely one of the best toons out there.
</p><p>
More later.  Really.  In the meantime, if you want meaningless, content-free
remixes of my pearls of wisdom, I refer you to the Markov version of my
blog, my <a href="http://toonbotting.blogger.com">Ulysses in the Caribee</a>.  It's
different, I'll give it that.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="google_silent.html">Google silent</a></div>
<div class="st2">2007-05-27 <a href="keyword_adsense.html">adsense</a></div><br>
I'm sure this will surprise no-one, but Google hasn't responded to my appeal as of today.
</p><p>
So I repeat: <i>don't</i> get into a position where you depend on Google for revenue.  If you think ad revenue is a good business model,
consider an ad partner who will avoid cutting you off for no apparent reason, and who will respond to questions if they do decide
to take some unilateral actions.  Google's too big and obviously overwhelmed with abuse -- but make no mistake; Google won't suffer
if somebody does something fishy on your site.  You will.  (Or at least, I can only assume that's what happened to me -- since
<i>Google doesn't have the ability to tell me</i>.)
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="ah_google_adsense_we_hardly_knew_ye!.html">Ah, Google Adsense, we hardly knew ye!</a></div>
<div class="st2">2007-05-18 <a href="keyword_adsense.html">adsense</a></div><br>
In an entirely unexpected development, Google has decided to terminate my Adsense account.  Not sure why, but my God!  Whatever shall I do without the
scads of money rolling in from my vast Internet empire?
</p><p>
Seriously, given that I wrote most of the content on this site years ago, it wasn't bad getting a few bucks for free every month, but it was strictly
recreational book-buying money. Nothing to write home about.  But tonight I am profoundly glad that I didn't make the time to bring in ad-revenue
traffic.  If I were depending on Google for my livelihood, I would be in a world of hurt with absolutely no recourse whatsoever.
</p><p>
Here's what their email said:
</p><p>
<blockquote>
</p><p>
Hello Michael Roberts,
</p><p>
It has come to our attention that invalid clicks and/or impressions
have been generated on the Google ads on your site(s). We have
therefore disabled your Google AdSense account. Please understand that
this was a necessary step to protect the interests of AdWords
advertisers.
</p><p>
As you may know, a publisher's site may not have invalid clicks or
impressions on any ad(s), including but not limited to clicks and/or
impressions generated by:
</p><p>
- a publisher on his own web pages
- a publisher encouraging others to click on his ads
- automated clicking or surfing programs, or any other deceptive
software
- a publisher altering any portion of the ad code or changing the
layout, behavior, targeting, or delivery of ads for any reason
</p><p>
These or any other such activities that violate Google AdSense Terms
and Conditions and program polices may have led us to disable your
account. The Terms and Conditions and program polices can be viewed at:
</p><p>
https://www.google.com/adsense/localized-terms?hl=en_US
https://www.google.com/adsense/policies?hl=en_US
</p><p>
If you have any questions about invalid activity or the action taken on
your account, please do not reply to this email. You can find more
information by visiting
https://www.google.com/adsense/support/bin/answer.py?answer=57153.
</p><p>
Sincerely,
</p><p>
The Google AdSense Team
</blockquote>
</p><p>
Wow.  I'm underwhelmed -- this gives me absolutely no information whatsoever as to what might have happened.  Now, since my daughter has been
in the hospital this week, I really haven't been online much at all.  So whatever <i>did</i> happen, I know it wasn't me.
</p><p>
So I thought, OK, I'll appeal it, what the hey?  Here's what the appeal form looks like (bolding is mine):
</p><p>
<blockquote>
</p><p>
As you know, Google treats instances of invalid clicks very seriously. By disabling your account, we feel that we have taken the necessary measures to ensure that invalid clicks will not continue to occur on your site. Due to the proprietary nature of our monitoring system, we're not able to disclose any specific details of these clicks.
</p><p>
Publishers disabled for invalid click activity are not allowed further participation in Google AdSense. However, if you can maintain in good faith that the invalid clicks we detected on your ads were not due to your actions or negligence, or the actions or negligence of others working for you, you may appeal the closing of your account.
</p><p>
<b>Google reserves sole discretion in considering whether to take any action on an appeal.</b>
</p><p>
In order to appeal the disabling of your account, please supply us with the details requested below. We're unable to consider appeals that do not contain <b>all of this information:</b>
 
<br>Name:
</p><p>
Company's name (If applicable):
</p><p>
AdSense Login (Email Address):
</p><p>
Publisher ID:
</p><p>
located in the AdSense code on your website with the format, pub-################<br>
URL:
</p><p>
Date Account was disabled:
</p><p>
Who are the intended users of your site?:
</p><p>
What is the source of your site's content?: 	
</p><p>
How often do you update your site?:
</p><p>
How do users get to your site? (How do you promote your site?):
</p><p>
How many people are involved with the administration of the site? :
</p><p>
Any relevant information that you believe would explain the invalid click activity we detected
</p><p>
Any data in your weblogs or reports that indicate suspicious IP addresses, referrers, or requests:
</blockquote>
</p><p>
And then the button for the appeal form: Submit.  Yeah.  Never has that default text seemed so appropriate.
But what sort of floors me is that the entire exchange is like this:
</p><p>
<i>Google</i>: We're cutting you off.  Have a nice day!<br>
<i>Me</i>: Wh-- what?  Who?  Why would you do that?<br>
<i>Google</i>: We don't know.  Do you have any relevant information which could explain our unilateral and arbitrary action?<br>
<i>Me</i>: Yeah.  I got your relevant information <i>right here</i>! 
</p><p>
A deeply disappointing experience.
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="some_initial_forum_spam_statistics.html">Some initial forum spam statistics</a></div>
<div class="st2">2007-05-13 <a href="keyword_spam.html">spam</a> <a href="keyword_forumspam.html">forumspam</a></div><br>
Now that I've been collecting spam from actual fora for a little while,
I have some initial statistics and musings.
</p><p>
I've collected spam from one eBoard 4.0 forum since May 5; it is now
May 13.  The spam filters I'm using are blocking about 93% of the postings,
making the moderation burden manageable for that forum.  In those 8 days
I have collected 1,235 spam samples.  That's 150 spams a day, from a fairly
obscure forum; in retrospect, even though the actual log activity seems
low, this is a lot of spam.
</p><p>
Those 1,235 spam samples link to a total of 10,795 links.  I haven't yet
built analysis machinery to get much farther than that; I've mostly been just
looking at the links, retrieving the pages, and musing about how all that
might be automated in an interesting and useful way.
</p><p>
Some tidbits:
</p><p>
Some of the spam links point to actual sites being advertised.  I don't yet
have a feel for many links point to sites <i>other</i> than those actually
advertised, but there are some interesting commonalities.  For instance, there
are a <i>lot</i> of pages placed onto vulnerable fora and other venues which
simply link to other pages.  In some cases, it's easy to tell why: Google
spamming and simply a way to counter attempts to block posts which link
to particular URLs.
</p><p>
I have a separate notion to find and track those vulnerable sites, and to
attempt to mine them for further information on these spam networks.
</p><p>
Bugzilla, oddly enough, seems to have such a vulnerability.  (Can you call this
a vulnerability?)  There are links to pages stored as attachments to bug
reports.  Those attachments are (naturally enough) not subject to any content
restrictions.  Unfortunately, that means you can put any Javascript into them
at all.
</p><p>
I haven't yet found actual malicious Javascript being spammed to fora.  What
I <i>have</i> found is obscured Javascript which modifies document.location to
force a page forward to another site.  I consider that semimalicious, and my
initial goal is to find a way to detect that with some sort of automatic
analysis, and block posts based solely on the basis of link to that sort of
page.
</p><p>
I figure it's only a matter of time, though, before I find some actual
malicious Javascript which will attempt to rootkit my machine with keyboard
loggers to steal my bank accounts.  That's pretty cool, actually, so I'm
watching the spam traps with bated breath.
</p><p>
One spam has a huge number of links to different domains, all of which resolve
to the same IP.  That's an interesting feature.  I'm not sure how to track it
yet.  What I really want to do is some kind of <i>generic analysis framework</i>,
but I don't have a good picture of what that framework would look like, or
indeed precisely what it is that I expect it to do.
</p><p>
It seems that what I want to do is to build a kind of task list for an incoming
event.  That task list would consist of a certain (small) number of analysis
steps which themselves generate new analysis events.  Each step is a test.
The results of the tests are cached, so that all possible duplicated effort
is avoided, but also so that relationships such as "these spam efforts share
an IP" can be found.
</p><p>
There's a certain exponential explosion involved, it seems at times.  But there
are also patterns which could cut down on the amount of work done.  Of those
10,795 links I have so far (oops, in the time it's taken to write this
much, two more spams have arrived, so I now have 10,886 links to analyze) --
of those 10,886 links I have, many of them are hosted at
<something>.007ihost.com -- 2,804 of them, as a matter of fact.  It will be
very interesting to analyze the spam pattern there, by the way.  Are all of
these from the same spammer?  Same IP?  (Bet not.)  But more germane to the
point I was making, eliminating those URLs from separate analysis will cut out
20% of the analysis effort.
</p><p>
Well, anyway, this is just a little talking out loud while I muse about how
to automate all this analysis.  Eventually I'll get down to posting graphs
of some sort.  That will be fun.  The other thing, of course, is some way
to ask about a URL, "Is this URL a spam indicator?"  I hope it will also
cross-fertilize with <a href="http://www.despammed.com">Despammed.com</a>.  Wish me
luck.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="modbot_google_counter_and_the_spam_archive.html">modbot, google counter, and the spam archive</a></div>
<div class="st2">2007-05-07 <a href="keyword_spam.html">spam</a> <a href="keyword_modbot.html">modbot</a> <a href="keyword_google_count.html">google_count</a></div><br>
Three related things today.  First, the scripts I've been putting together for forum spam blocking have kind of coalesced into a "modbot".  This
program attempts to automate the tasks performed by human moderators and could technically be placed into <i>any</i> Web spam moderation situation.
It is currently running happily in an Eboard 4.0 installation and blocking roughly 93% of spam, while still allowing anonymous posting to that forum.
I'll be packaging it up for distribution in the public domain.  Watch this space for further details -- one of the more fascinating notions I've
had is to enable it to receive moderation emails from Blogger and thus automate the comment moderation process there.
</p><p>
One of the rules/tools used by the modbot is to count Google hits for the numeric IP of an untrusted poster.  Turns out that HTTP proxies have a
real proclivity for getting indexed.  A lot.  Legitimate IPs, not so much.  I wrote a little online tool to call Google to get these counts; the
<a href="/google_count.html">tool is here</a> and the <a href="/projects/forum_despammer/google_count.html">write-up of the code is here</a>.  It's currently blocking about
40% of spam (I don't have good statistics analysis in place yet, so that's very approximate.)
</p><p>
Finally, as a spinoff of this project, I've started a spam archive.  There's nothing to present yet, but I hope to start doing some interesting
analysis, and most specifically a searchable database -- along with a searchable database of spamvertised sites.  That ought to overlap with
the sites spamvertised by email spam as well, and that's going to be an interesting thing to look at.  We'll see.
</p><p>
I've stumbled onto a spam link network of staggering extent in the course of examining forum spam.  A spammer has a site somewhere, and then spamvertises
it.  But then some of the spam starts to link to other forum spam, which in turn links to the site.  Some sites auto-forward to other sites using
obscured Javascript (I haven't figured out just why, yet; if you have a rationale, I'd be happy to hear it.)  Anyway, after that goes on for a while,
there's a huge resulting network of vulnerable fora linking to other vulnerable fora.  There is a true treasure trove of information available to
the interested party.  Which would, of course, be me.  I will <i>definitely</i> be following up on that and posting on it.
</p><p>
Anyway, it's been nice talking to you.  Back to work!
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="free_xrumer__honor_among_thieves.html">Free XRUMER?  Honor among thieves</a></div>
<div class="st2">2007-04-25 <a href="keyword_xrumer.html">xrumer</a></div><br>
In the predictability department, one of my forum spam traps just pulled in
an interesting post: yeah, it was posted (presumably) by XRumer and certainly
fits the profile -- but it's advertising a crack of XRumer.
</p><p>
Heh.
</p><p>
"Greate new XRumer4.0 platinum edition and crack DOWNLAUD".
</p><p>
I wondered how long <i>that</i> cash cow would last -- looks like about, what,
November to April?  Actually, it took longer than I expected.
</p><p>
In case you're wondering whether this is a good idea, well, given that you
therefore think spamming is a valid business technique, then: sure, go ahead.
Download a crack from Russians and <i>give</i> them control of your machine.
</p><p>
In related news, I have doubled the number of forum sites I am despamming.
(If you're paying attention, that means, yes, I now have one that isn't my
own site.)  And I decided to try a notion that's really paid off in spades.
</p><p>
See, XRumer uses a vast database of known HTTP relays to post spam.  This
makes it much more difficult for human admins to block by IP -- since a single
spammer may have hundreds of IPs available, how can you block?
</p><p>
Well -- unintended consequence time!  Thanks to the explosion in use of these
proxies, we now have a reliable way to find them out without human intervention
at all.  Count the number of times Google indexes an IP, and you have an
<i>incredibly</i> effective way to determine whether it is on the list of known
proxies used by spammers.  Granted, you have the lag between the time it
becomes a proxy and when Google starts indexing the references to it on forum
posts around the world.  But this one test for spam blocks about 60% or more
of forum spam, sight unseen.
</p><p>
It won't last.  But then again, neither will XRumer, not in its present form.
</p><p>
Just to help you out, I've provided a simple Google hit counter:
<a href="http://www.vivtek.com/google_count">go here and type in any phrase</a>, not just
an IP address, to see how many references to the phrase Google has indexed.
When I've got a little more timeframe behind it, I'll even put in autorepeating
queries of the good ones, with gnuplot graphs to show googlecount over time.
</p><p>
And of course, I'll be putting the code up; it's about ten lines of Perl --
the only reason it's that long is that it caches results in a database so
repeated queries don't pound Google.  Not that Google can't stand the pounding,
but I don't really want a bunch of Perl script threads hanging around waiting
on Net latency.
</p><p>
So, a common refrain lately: more later.
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="mild_cognitive_dissonance.html">Mild cognitive dissonance</a></div>
<div class="st2">2007-04-06 <a href="keyword_xrumer.html">xrumer</a></div><br>
Second post in a day...  Turns out that the WaPo posted on XRumer back in January.
<a href="http://blog.washingtonpost.com/securityfix/2007/01/scary_blogspam_automation_tool_1.html">The article is here, with comments.</a>
Note that the comments are, except for four, <i>all by Russian spammers</i>.  Who are tagging the Washington Post with high-fives
because they've caught the attention of the mainstream.
</p><p>
If that doesn't blow your pretty little mind, I'm not sure what will.  I love this century!
</p><p>
So again: I'll help you block XRumer if you want.  Just <a href="mailto:michael@despammed.com?subject=XRumer%20despamming">drop me a line</a> and
we'll talk.  This ought to be fun.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="xrumer_-_they_re_back!.html">XRumer - they're back!</a></div>
<div class="st2">2007-04-06 <a href="keyword_xrumer.html">xrumer</a> <a href="keyword_spam.html">spam</a></div><br>
So hey, kids, I'm still alive, and now posting from the lovely Caribbean island of Puerto Rico for the foreseeable future.
</p><p>
After the move, and after some confusion on the part of the cable company involving <i>losing my order</i>, I have blessed, blessed broadband
again, without having to cadge the neighbors' WiFi from the rooftop terrace, which would be a great place to work were it not for the tropical
proximity of a horrible huge ball of blazing nuclear explosion hanging over my head, plus the necessity of placing the laptop in a
precarious position on the railing, four floors above concrete, to get good signal.
</p><p>
But now things are good again, and I have 9000 emails to go through (yes, as a guy with a spam filter, I should probably be filtering my spam, but,
well, it's a long story and <i>look, shiny thing!</i>).  And lo! within those 9000 mails were two from hapless forum operators who are getting fed
up with manual despamming.
</p><p>
So sure, I'll be seeing what I can do in that regard, but it piqued my interest in forum spam again.  And so I checked my logs for instances of
XRumer, and wow -- somebody actually <a href="http://kenya.rcbowen.com/talk/viewtopic.php?id=8545">linked my XRumer blog keyword</a> in response to
... a new instance of the XRumer forum bomb.  Dated April 5, as
it so happens.  This one contains the novel text "Also, do you know when XRumer 4.0 Platinum Edition will be released?" and it's posted by AlexMrly.
Google either the phrase or the name, and you'll see a whole lot of forum spam.  Hey, XRumer guys -- thanks!  What we all want is more forum spam!
</p><p>
Now I have that off my chest.  I'm going to reiterate my offer to anybody listening -- I'm going to see what I can do to combat forum spam around
the world, and I'm not charging anything for it.  So far, I'm just in it for the interest, just like email spam in 1999.  Get in touch.  I'll be here.
Well -- I might actually be at the beach.  But I'll be back soon.
</p><p>
Sorry that this post isn't really all that programming-oriented.  I hope to be making that right, in the next couple of days.  Blocking XRumer is
fun, and so easy even a child could do it!  No, seriously: if you want to help me stop XRumer, all I need is your data.
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="no_time_to_write!.html">No time to write!</a></div>
<div class="st2">2007-03-01 <a href="keyword_blogmeta.html">blogmeta</a> <a href="keyword_autohotkey.html">autohotkey</a> <a href="keyword_sqlite.html">sqlite</a></div><br>
Just so I don't forget how this blog thang works, I want to assure each and every one of you bots who reads this blog that
I've been doing lots of cool programming-type stuff...  Well, OK, actually, the family and I went sledding every day for a week
and a half while the weather was right, and I've been scrambling to finish up a hueueueuge job that I was neglecting during
that time.  So ... no excuse at all.
</p><p>
On the translation front, I've discovered that hotstrings (e.g. Word's AutoCorrect feature) can really speed up my typing
in cases where I'm doing repetitive texts.  Even the time savings of typing "s" instead of "SAP" can really add up over time.
But Word's AutoCorrect has a problem -- it saves different lists for each style, and in some instances it triggers without waiting
for me to finish the word; if one of my hotstrings is a prefix of another word, that can truly suck.  Some Googling got me
to <a href="http://www.autohotkey.com">AutoHotKey</a> -- and AutoHotKey truly and totally rocks.  A lot of what I wanted to do with PyPop is already
done and ready for me to use, actually.  So I'm going to start bundling AHK with PyPop for Windows systems.  It's that good.
</p><p>
Another nice discovery this week has been <a href="http://www.sqlite.org">SQLite</a> -- which is not just open-source, it's actually <i>public
domain</i>.  It implements pretty much all of the SQL92 standard (except the permission model) for a lightweight local database for
single-user use.  Websites have been built on it with impressive performance.  And the key is -- you can bundle it into anything.  Anything.
So it's definitely going into <a href="/wftk/">the wftk</a>.  Man.  I'd been kind of moving towards building my own SQL parser and so on -- what's the
point?  It's already been done!  And beautifully!
</p><p>
So there's life in me yet, never fear.
</p><p>
On that note, it's back to work for me.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="malware_analysis_and_generic_file_reading.html">Malware analysis and generic file reading</a></div>
<div class="st2">2007-02-13 <a href="keyword_malware.html">malware</a> <a href="keyword_dataparser.html">dataparser</a> <a href="keyword_security.html">security</a> <a href="keyword_programming.html">programming</a></div><br>
For many moons, I've had this crazy idea of a generic file parser floating
around in my head.  (The idea, not the parser.)  This would function a lot
like a hex editor, except that it would operate on a semantic level: if
an extent in the file was known, meaning that its purpose was known or at
least guessed at to the point where it could be named, then that information
would be marked in a file description.
</p><p>
An example of this would be a malware analyzer.  In case you haven't seen
the term before, "malware" is software that is out to do you harm.  Viruses,
worms, and stuff like that.  A popular source of malware is executables
attached to email in such a way that Outlook will execute it without asking
you.  Yes, this still happens.  For lots of details of this kind of
exploit, see <a href="http://isc.incidents.org/">the Internet Storm Center's blog</a>.
Hours of fun reading there.  No, seriously!  Malware is fun!
</p><p>
But as any reader of this humble blog knows (both of you), my time for fun
is strictly limited, and my patience wears thin very quickly.  So I never
actually analyze any malware, because to do so I'd probably have to find
a piece of paper and note stuff down.  Hence the need for software to do it
for me: if I had something that defined the sections of an EXE file under
Windows, for instance, then I'd run that against the malware, and I'd at least
break everything down into conveniently readable chunks -- I'd eliminate the
EXE header, split out the resources, that kind of thing.
</p><p>
This, then, is a generic file parser.  It allows me to <i>interactively</i>
define a file structure for a given file (or class of files) and read useful
data out.
</p><p>
A more proximate reason to do this, lately, has been that I have a need to
use a glossary file which has resisted import into MultiTerm, my glossary
software of choice.  I could open a hex editor and <i>see</i> the terms,
but I couldn't do anything useful with them.
</p><p>
Well, yesterday I spent the whole day on it, but I have a prototype of said
file parser.  Using it, I can define hexblocks, sequences, lists, records,
switchable sections based on flags, variable-length blocks based on length
specifications in the file -- all that works like a charm, and I get a
nice, readable dump file for my trouble.  As I refine the file description,
I get more readable dumps.  And then I can write a Perl script to scan the
dump and pull out whatever I like.
</p><p>
It is so extremely useful.  Unfortunately, I only slept about three hours
last night, since I stayed up until 3AM coding and didn't actually do the
paying work I should have been doing instead...  So posting this project
will have to wait for another day -- but once it <i>is</i> posted, wouldn't
it be groovy to have an interactive online file parsing tool for, say,
malware snagged off the wild Net?  That would be fun!
</p><p>
So: more later.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="web_2_0.html">Web 2.0</a></div>
<div class="st2">2007-02-07 <a href="keyword_art.html">art</a></div><br>
You want to see something nice?
</p><p>
<a href="http://www.youtube.com/watch?v=6gmP4nk0EOE&eurl=">Go watch this.</a>
</p><p>
The Machine is us.  That's exactly what I've been trying to express
for years.  And this expression of that insight is quite artistic as well.
Which just reflects another pillar of my philosophy -- beauty counts.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="blog_topic_tag_cloud_weighted_by_traffic.html">Blog topic tag cloud weighted by traffic</a></div>
<div class="st2">2007-02-05 <a href="keyword_traffic.html">traffic</a> <a href="keyword_tag-cloud.html">tag-cloud</a> <a href="keyword_perl.html">perl</a></div><br>
This is something I've wanted to do for a couple of weeks now -- I have a handy set of scripts to filter out chaff from my hit logs, and to grep them
out to convenient category files (like "all interesting non-bot traffic to the blog").  So I've written a script to take all that blog traffic and
determine which tag it should be attributed to.  Hits to individual pages boost the traffic to all their tags.
</p><p>
The resulting tag cloud is on the <a href="/blog/kw/">keyword tag cloud page</a> next to the cloud weighted by posts.  This is a really meaningful way to analyze
blog traffic and get a feel for what people are actually finding interesting.  A possible refinement might be to time-weight the hits so that more
recent hits count for more weight (that would be pretty easy to do, actually -- even so cheesily as to count number of hits and multiply all the counts
by 90% for every ten hits or something.)
</p><p>
The Perl code to read the logs and build the cloud file is below the fold.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="xrumer_s_popularity_continues_unabated.html">XRUMER's popularity continues unabated</a></div>
<div class="st2">2007-02-05 <a href="keyword_spam.html">spam</a> <a href="keyword_xrumer.html">xrumer</a></div><br>
When I initially posted the XRUMER and you post, I thought that XRUMER
probably used the text I posted (which I had found on a forum I frequent)
to identify spammable fora -- those for which moderation is not performed.
</p><p>
Later, I came across the theory that this post was in fact some pretty
clever viral marketing.  By pretending to ask the forum's members about
XRUMER, the XRUMER marketer could induce at least some people to search on
it and link it, causing Google to rate it highly without actually themselves
spamming.  Neat.
</p><p>
But for whatever reason, my post caused Google to rate <i>me</i> third on
searches on the term XRUMER -- and instead of XRUMER, I'm seeing a lot of
traffic from people obviously interested in <i>stopping</i> it.
</p><p>
As am I.
</p><p>
But I don't have access to a forum affected by XRUMER (or at least, I can't
tell for sure that I do.)  My own Toonbots forum is an extremely low-traffic
venue running on antiquated WebBBS code.  I get spam there, and this week
<a href="/projects/forum_despammer/">managed to block it all</a> (so far), but my problem
is decidedly minor.
</p><p>
I can only assume that if you're reading this, you have a major forum spam
problem.  If this is the case, I need your help.  I'd like to try out some
ideas about forum despamming -- building on the working concepts in my own
low-traffic venue.  But to try these ideas out, <b>I'd need access to a forum.
Your forum,</b> if you're interested.  And that essentially means access to the
underlying storage (whether filesystem or database), a way to run Perl on your
box, and access to the Web access logs in real time.
</p><p>
Depending on your own traffic patterns, the access logs can provide a great
deal of information about whether a post is legitimate or not.  Of course,
you can also make a lot of valid judgments based on the post content, but I
hesitate to block on things like "too many links," as satisfying as that
heavy-handed approach may be.  Legitimate users can often have legitimate
reasons to post lots of links.  Granted, they're generally not about Cialis
or mortgages or hot xxxxxxx Asian lesbian pr0n, but still -- any interference
with your actual users is something you want to avoid at all costs.  I regard
information about post content to be one factor in a good, well-rounded
spam elimination strategy.
</p><p>
Traffic analysis correlated with forum activity can be a powerful tool, and
in my own case it's working 100%, with no examination of content at all, but
my traffic is so low that I can't judge how complete a strategy it might be.
If you add your forum to the mix, I can improve the techniques.
</p><p>
So anyway, all you desperate forum admins with XRUMER problems -- if you want
me to give it a shot, drop me a line.  I'm working for free and during an
initial phase my scripting can simply recommend post deletion instead of making
any automated changes itself.  Interested?  <a href="mailto:michael@despammed.com?subject=XRUMER%20despamming">Tell me.</a>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="despamming_the_toonbots_forum.html">Despamming the Toonbots forum</a></div>
<div class="st2">2007-02-04 <a href="keyword_spam.html">spam</a></div><br>
I just wrote a rather effective spam eliminator for my WebBBS forum at Toonbots, and sort of "live blogged" the process as I went.
The result is a <a href="/projects/forum_despammer/">rather attractive little document</a>.  I feel virtuous again tonight.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="pypop_v0_1_released.html">PyPop v0.1 released</a></div>
<div class="st2">2007-02-02 <a href="keyword_wxpywf.html">wxpywf</a> <a href="keyword_pypop.html">pypop</a> <a href="keyword_appaweek.html">appaweek</a> <a href="keyword_code_generators.html">code_generators</a></div><br>
Finally!  I've been pretty busy with the paying work this last week,
and also with biochemistry due to my son's kidney/allergy problems, and so
lowly open-source work has suffered.
</p><p>
But the PyPop GUI framework is ready to download in a convenient NSIS
installer.  Rather than host it, I've put it up onto the
<a href="http://sourceforge.net/project/showfiles.php?group_id=26205&package_id=220313">the SourceForge download page</a>
for your downloading pleasure.
</p><p>
Once it's installed, download the <a href="/projects/filetagger/filetagger.wftk">filetagger app definition</a> and play around with it.
It's all still pretty crude, but I'm having fun.  Did I mention that this actually involves the on-the-fly generation of a Python
class based on the XML application definition, which is then instantiated in the GUI to do the work?  That was fun!
</p><p>
Anyway, more later.  I'm still on week #3 of the app-a-week thing, for, um, the second or third week.  Maybe I'll slowly approach
an app a week as I get this stuff under control.  Wish me luck!
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="filetagger_posted.html">Filetagger posted</a></div>
<div class="st2">2007-01-28 <a href="keyword_filetagger.html">filetagger</a> <a href="keyword_wxpywf.html">wxpywf</a> <a href="keyword_appaweek.html">appaweek</a></div><br>
I posted v1.0 of the filetagger in the new PyPop format.  The XML definition
of the app is 310 lines and about 12K.  I think this could end up being
quite useful.
</p><p>
The code is <a href="/projects/filetagger/filetagger.html">here</a> -- I don't
have the actual running PyPop up to run it, though.  I still want to get
registration of file extensions working -- oh, yeah, and what there is
of the help system.  The help text is included but there's no command to
display it yet.
</p><p>
If I end up defining a basic XSLT processor on top of the XMLAPI, this
could start to get really interesting...
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="file_tagger_app_finished!.html">File tagger app finished!</a></div>
<div class="st2">2007-01-11 <a href="keyword_filetagger.html">filetagger</a> <a href="keyword_wxpython.html">wxpython</a> <a href="keyword_wxpywf.html">wxpywf</a> <a href="keyword_appaweek.html">appaweek</a></div><br>
At long last, I managed to finish development on a first cut of the <a href="/projects/filetagger/filetagger1.html">filetagger application</a>.
It took <i>far</i> longer than I really wanted it to, because I spent an inordinate amount of time whipping the
<a href="/wftk/doc/code/python/wxpywf.html">wxpywf framework</a> into shape (about a month) and so the whole "app a week" thing is more like
"an app per five weeks" or so.  Ha.
</p><p>
But you know what?  <i>I did it!</i>  I actually brought a major new module of the wftk, one I'd been thinking about for three years,
to the point where it can be used.  Wow.
</p><p>
So I'm glad I took the time to do it the way I wanted to do it.
</p><p>
Here are some of the features of wxpywf I created and used for this app:
</p><p>
<ul>
<li>XML definition of the entire UI of an application, using frames and dialogs.  In comparison with the traditional call-by-call
    technique for setting up a wxPython UI, this is incredibly convenient.
<li>Application-specific code grouped into simple commands.
<li>Each frame and each dialog automatically binds to an XML record which can be
    addressed on a field-by-field basis.
<li>HTML can be used for more textual interfaces; links generate commands which can
    have arbitrary effects on the UI (in this case, clicking on a link in the tag cloud
    switches the tabbed frame to the file list and displays the files with the tag
    selected.)
<li>So far, the UI can include tabsets, list controls, HTML windows, rich text controls,
    checkboxes, radio button groups and listboxes, command buttons, and static text.
</ul>
</p><p>
There's a lot of ground still to cover.  But in my experience, that kind of ground can be covered in small, manageable steps
after initial usability is there.  And initial usability is definitely there.  I feel really happy about this.
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="forum_spam.html">Forum spam</a></div>
<div class="st2">2006-12-31 <a href="keyword_spam.html">spam</a></div><br>
Over at Toonbots, I have a <a href="http://www.vivtek.com/toonbots/discuss.pl">forum</a>, based on ancient
but reliable Perl code.  For many years, that forum has been a quiet backwater
of the Net where I chat on various topics with those of my friends who enjoy
that facet of my personality responsible for the engendering of Toonbots.
</p><p>
But lately, something extremely irritating has happened.  The forum has become
the target of forum spammers.  Their spam rarely even formats correctly, since
the forum code is so old and weird.  But that doesn't stop three or four of
them from posting every day, and I have to delete it all by hand, or relinquish
the forum to utter uselessness.
</p><p>
Oddly, the <a href="http://www.vivtek.com/wftk/discuss.pl">wftk forum</a> is utterly unaffected by all this.
Since the trouble started when I started properly indexing the forum archives,
I suspect the archives are acting as a Google magnet for various topics.
But I'm not sure yet.
</p><p>
The modus operandi of forum spammers is different from real posters, according
to the logs; typically the forum spammer hits the site for the first time in
the forum archives, then posts within a few seconds.  Real posters actually
read the site first.  So I <i>could</i> filter based on that behavior.  But
I'm going to study the issue for a while, see if I can detect any other
useful patterns.  It's a serious problem, and a growing one; email spamming
is experiencing diminishing returns now, since fewer people read email thanks
to spam.  So forum spamming is a logical progression.
</p><p>
Jerks.
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="the_reanimated_zombie_corpse_of_despammed_com.html">The reanimated zombie corpse of Despammed.com</a></div>
<div class="st2">2006-12-27 <a href="keyword_despammed.html">despammed</a> <a href="keyword_spam.html">spam</a></div><br>
Once, many years ago, I did a foolish thing.  I wrote a quick little spam filtration forwarder and opened it up to the public.  As far as I can tell,
I was the first person to have done so.
</p><p>
The year was 1999, and the Boom was in full swing.  My plan was simple: (1) write a free online service, (2) ???, and (3) profit!!!  As you no doubt
can surmise, #2 never really happened, let alone #3.  But from 1999 to sometime in 2005, I kept that thing running, through three servers, four household
moves, and a growth in userbase from two to a few thousand (if I recall correctly).
</p><p>
Then: the server died.  I mean, it died suddenly and irretrievably, and I (never much of a stickler for formalities) had not backed it up.  Ever.  I'd
simply moved it from place to place while <i>intending fully</i> to back it up, and all the old machines were broken into their constituent materials
by that time.  As I was going through some
serious financial woes, and as my wife and I had found that our son has a kidney disorder, my priorities were clear.  Despammed.com was superfluous.
</p><p>
But it nagged at me.  Despammed.com lived on in my heart, even though its DNS entries pointed to an IP now occupied by some calendar service thing.
(Which was weird.)  And then I found a not-too-dusty copy of the HTML.  And last month I found relatively good copies of the filtration software.  I still
don't have the user database or the administration/registration code or the statistics or the filter databases or, really, anything.  But what the heck.
I <a href="http://www.despammed.com">put it back online anyway</a>.  I'm nearly positive I'm going to live to regret it.
</p><p>
But I still feel a warm holiday glow.  I'm giving back to the community again.  Merry Christmas to all of you!  And if Despammed.com breaks into your
house wanting to eat your brains, remember: a headshot is mandatory.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="xslt_anyone.html">XSLT, anyone?</a></div>
<div class="st2">2006-12-22 <a href="keyword_xslt.html">xslt</a> <a href="keyword_test-based-programming.html">test-based-programming</a></div><br>
Sunday, I translated Chapter 12 of a
<a href="http://www.sappress.com/product.cfm?account=&product=H1934">book on ABAP programming under SAP</a>,
forthcoming in the English edition from SAP Press in March.  This was
actually the third chapter I'd done in this book, and the others were
a little far afield from what I usually do, since I don't actually do
SAP work (except from a translation standpoint.)
</p><p>
But this chapter was fascinating, involving the external
interactions of the SAP system with other systems.  One of the supporting
technologies of those interactions is SOAP, which brings up XML, which in
turn brings up something I have never particularly paid much attention to:
XSLT.
</p><p>
Wow.  I didn't know what I was missing!  XSLT is essentially a template-based
programming language, used to transform XML structures into other XML
structures; the idea is to support the expression of data-centric XML into
more presentation-centric XML, but the mechanism is suitably generic.
</p><p>
And you know what?  There's no open-source XSLT processor written in C.  And
this is odd, because even though the XSLT spec, like everything produced by
the W3C, is horribly opaque, <i>really</i>, when you get down to it, XSLT
is a pretty straightforward little language, and the parser is a no-brainer
because <i>it is already expressed in XML</i>.  So half the hard work in
writing a compiler is already done for you, right there.
</p><p>
So here is my latest exciting brainstorm: I want to set up a test-based
environment to support the development of an XSLT processor in C.  Each test
would consist of a starting structure, the template or templates to run on it,
and the expected result -- that's easily imagined.  And each test also
refers to the exact point in the W3C spec that requires it (and vice versa
-- I'd have an annotated spec that refers back to the tests.)
</p><p>
The whole thing would then function as an "XSLT by examples" tutorial and
also a testbed for a command-line XSLT processor.  Wouldn't that be a nice
thing for the world to have?
</p><p>
I'm all for starting tomorrow.
</p><p>
<i>Update:</i> Whoops, I guess mod_xslt2 for Apache is in C, looks like.
Well, never mind, I still think it's a bang-up idea.  We'll see what the
New Year brings, eh?
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="thoughts_on_wxpython_ui_design.html">Thoughts on wxPython UI design</a></div>
<div class="st2">2006-12-22 <a href="keyword_appaweek.html">appaweek</a> <a href="keyword_wftk.html">wftk</a> <a href="keyword_wxpython.html">wxpython</a> <a href="keyword_tagger.html">tagger</a></div><br>
For some time, in the context of my <a href="/wftk/">workflow toolkit</a>, I've been thinking intensively about UI design in wxPython.
</p><p>
See, once I was embroiled in a rather extensive project developing a GUI application under wxPython, and frankly, the UI was unmanageable.
It had been developed with some IDE tool or another, but the output was <i>Python code</i>.  It was horrible, trying to find what was what
and on which panel it was developed and what its ID was -- ugh!  This was back in about 2001.
</p><p>
At that point, I hadn't really started integrating wftk into Python yet, but I dabbled in it over the next couple of years, always with the notion
that the UI is most sensibly defined in XML, and that a sensible UI manager would then take that definition and build all the objects needed to
implement it in wxPython (or, for instance, online in a portal or something).  And since that time, other people have naturally had many of the
same ideas, and you see this implemented.  But I've always wanted to finish my own implementation.
</p><p>
The current app for <a href="http://www.anappaweek.com/2006/12/requirements-for-file-tagger_21.html">Anappaweek.com</a> that I'm working on is, of course,
a GUI app (at least, some of the time.)  And so naturally I have relived my need for my UI design notion -- and in the context of working on the
file tagger, I intend to start implementing the UI module.  On that note, here is a tentative UI definition sketch for the file tagger.  Ideally, we could
use this XML not only to generate the app itself, but also to generate documentation for the UI design (by transforming it with XSLT into SVG, for instance;
wouldn't that be indescribably cool?)
</p><p>
<i>All</i> of this is, of course, subject to radical change.  Here goes:
</p><p>
<pre>
&lt;frame&gt;
  &lt;tabset&gt;
    &lt;tab label="Cloud"&gt;
       &lt;html&gt;
    &lt;/tab&gt;
    &lt;tab label="Files"&gt;
       &lt;splitter (some kind of parameters)&gt;
         &lt;panel&gt;
           &lt;radio-group&gt;
              &lt;radio value="something" label="All"/&gt;
              &lt;radio value="something" label="Some"/&gt;
           &lt;/radio-group&gt;
           &lt;listbox/&gt;
           &lt;button label="Show"/&gt;
         &lt;/panel&gt;
       &lt;listcontrol&gt;
         &lt;col label="Name"/&gt;
         &lt;col label="Tags"/&gt;
         &lt;col label="Description"/&gt;
       &lt;/listcontrol&gt;
    &lt;/tab&gt;
  &lt;/tabset&gt;
&lt;/frame&gt;
</pre>
</p><p>
I already have a framework for that definition to go into -- I wrote that in, like, 2002 or so.  But I never got further than definition of menus.
So here, I'm going to implement frames, and at least one dialog.
</p><p>
Note that what's utterly missing from this is any reference to code to handle events.  That will come later, when I see what has to be defined where
to get all this to work.
</p><p>
And on that note, I close.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="xrumer_and_you.html">XRUMER and you</a></div>
<div class="st2">2006-12-17 <a href="keyword_spam.html">spam</a> <a href="keyword_xrumer.html">xrumer</a></div><br>
(coff)
</p><p>
Need overview about XRumer software?<br>
I'm seeking for any information about <b>XRUMER program</b>. <br>Can you help me? Or give me a link to the official site with this autosubmitter.
</p><p>
There.  Now let's wait a week for Google to index this, and see what the log drags in.  Thank you very much, and I now return you to your regularly
scheduled programming.
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="special-purpose_text_editor_1_complete.html">Special-purpose text editor #1 complete</a></div>
<div class="st2">2006-12-12 <a href="keyword_appaweek.html">appaweek</a> <a href="keyword_text-editor.html">text-editor</a> <a href="keyword_wxpython.html">wxpython</a> <a href="keyword_programming.html">programming</a> <a href="keyword_litprog.html">litprog</a></div><br>
So my first actual weekly application is finished now.  Aren't you proud?  Suffice it to say that
even a minor app takes a few hours to put together when you're reworking all your programming
tools at the same time.  A character flaw, I suppose.  I never use an already-invented wheel
if I have a perfectly good knife and wheel material.  And I never use an already-invented knife
if I have a perfectly good grinder and stock metal.  And I never use an already-invented grinder
if I have a lathe, motors, and a grindstone.  And I never use an already-invented lathe...  (sigh).
</p><p>
At any rate, it took me a few hours more than I wanted, but I'm reasonably pleased with the
result.  You can see the whole thing
<a href="/projects/text_editor/text_editor1.html">here (it's far too long to publish on the blog directly, of course)</a>.
Go on.  Look!
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="my_new_translating_technique_is_unstoppable.html">My new translating technique is unstoppable</a></div>
<div class="st2">2006-12-08 <a href="keyword_translation.html">translation</a> <a href="keyword_machine-translation.html">machine-translation</a></div><br>
I just wanted to note at this juncture that my notion of running some very
simple machine-translation code (yes, lovingly hand-coded in Perl) on a
certain class of text, followed by human intervention using <i>just the
right kind of editor</i> seems to be bearing fruit.
</p><p>
Granted, I would be in much less deadline trouble right now if I'd just
done the Right Thing, shut up, and translated the text.  But the text in
question is not nicely flowing text.  It is PLC message output written
by engineers for machine operators, and it is <i>dense</i>.  Very, very
dense.  So if I'd just translated it by hand I would have screwed up over
and over.
</p><p>
Instead, I first scanned the entire text, broke it into words (with varying
success), and looked up each and every word I didn't know.  For those of you
who aren't translators, this doesn't just mean not knowing a word at all; it
includes not knowing what those particular engineers and machine operators
intend to say with a particular technical term.  This can be challenging, but
in this case I had a lot of previously translated text, so I could look most
words up in that.
</p><p>
Once all the words were "known" (ha), I ran the whole thing through a phrase
scanner.  Frequently occurring phrases were presented with word-by-word
translations, along with some crude rewrite rules to make a better guess.
This is all very, very naive, as any translator knows.  It's not even as good
as SYSTRAN, and SYSTRAN sucks.
</p><p>
But as I translated more of the frequent phrases, the system was able to
string together better guesses for the longer phrases.  At some point, then,
I decided to switch over to direct translation of the actual segment list.
This text was "nice" (in this one aspect alone) because segmentation was
easy -- every line is a separate sentence, so there's no need to figure out
where sentences might break.  That's convenient.
</p><p>
At any rate, I am now using my specialized text editor to approve and/or
modify each resulting phrase.  Remember: all the words are already there,
sort of, just not usually in an understandable order.  Now that I can <i>very
quickly</i> select and drag them around, though, <b>my new translating
technique is unstoppable</b>. Ha!  They said it couldn't be done!  <i>Those
fools!</i>  MWAahahahaha!
</p><p>
(coff)  OK, I'm better now.  Documentation soon.  I just felt enthusiastic.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="writing_specialized_text_editors_in_wxpython.html">Writing specialized text editors in wxPython</a></div>
<div class="st2">2006-12-07 <a href="keyword_wxpython.html">wxpython</a> <a href="keyword_text-editor.html">text-editor</a> <a href="keyword_translation.html">translation</a></div><br>
So again, a lick and a promise for this blog as I madly try to finish some
translation work.  This translation job is an interesting one, though, as
I mentioned, and as it turns out, amenable to editing in a specialized
tool I just wrote today.  Of course, having <i>written</i> the tool today
means I have to <i>use</i> the tool this evening in a mad dash to finish,
which in turn means I have no time to <i>document</i> the code until
tomorrow at the very earliest.
</p><p>
Suffice it to say that the exercise was surprisingly easy.  The task was
simple: I need a tool to edit text files in which (for reasons we'll go into
later) I have a number of phrases, one per line.  The phrases mostly have
all the right words in them, but not in the right order.  I thus need a way
to <i>quickly</i> select one or more words and drag it into the right place
in the phrase.  Sure, you say, Word does that.  Yeah, except that Word
doesn't put the spaces in the right place.  God and Bill Gates alone know
why, but <i>Word doesn't put the fricking spaces in the right place</i> when
I drag words around on a line, and so I took matters into my own hands and
rolled my own solution.  And by God it works!  Still a few little oddities
in it, but it works more quickly than Word for this particular application.
</p><p>
Another nice thing it can do is this: when I drag the first word of a phrase
out into the middle, it can decapitalize that word, and capitalize the new
first word.  That saves me a fraction of a second, and multiplied by 2000
phrases that adds up to a lot of time.
</p><p>
And another little thing I just now added: I can hit a key and toggle the
case of the word the cursor is on.  Again: this may or may not be of general
use, but for this particular application it's very convenient.  And that's
really the idea of special-purpose text editors.  An example from the
programming world is emacs -- you can write LISP code to make emacs do
literally anything at all (including psychoanalysis) from your text editor.
The only problem being that it's too damn hard to start.  Python's easier,
at least for me.  So a text editor in which you can embed your own Python
snippets might be a generally useful tool indeed!
</p><p>
So.  Tomorrow or Sunday, documentation and maybe some more movement on the
drop tagger.  And in the meantime, go get some sleep!  (I know I won't any
time soon.)
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="drop_handler_per_se.html">Drop handler per se</a></div>
<div class="st2">2006-12-06 <a href="keyword_drop-handler.html">drop-handler</a></div><br>
Whew.  Take a look <a href="http://www.codeproject.com/shell/ShellExtGuide6.asp">here</a>
if you want some complicated stuff.  This is a guide written for
complete idiots ("The Complete Idiot's Guide to Writing Shell Extensions")
and I'm on my third time through it.  Apparently I'm not a complete idiot.
</p><p>
My mission, if I choose to accept it, is to <i>do that</i>, in Python.
It should be possible, but I have a translation deadline, um, yesterday,
and so I don't have much more time than to note that link, globber something
to the effect that "I'm soooo confused", and move on.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="tag_file_clouds.html">Tag/file clouds</a></div>
<div class="st2">2006-12-04 <a href="keyword_appaweek.html">appaweek</a> <a href="keyword_tagger.html">tagger</a> <a href="keyword_tag-cloud.html">tag-cloud</a> <a href="keyword_blogmeta.html">blogmeta</a> <a href="keyword_perl.html">perl</a></div><br>
Today's fun task was the creation of a little prototype code to format the tag cloud for the drop handler project.  I did it
in the context of this blog, and so first I had to get my keywords functional.  I already had a database column for them,
but it turned out my updater wasn't writing them to the database.  So that was easy.
</p><p>
Once I had keywords attached to my blog posts, I turned my attention to formatting them into keyword directories (the primary
motivation for this was to make it possible to enable Technorati tagging, on which more later.)  And then once <i>that</i> was
done, I had all my keywords in a hash, so it occurred to me that I was most of the way towards implementing a tag cloud
formatter anyway.
</p><p>
Here's the Perl I wrote just to do the formatting.  It's actually amazingly simple (of course) and you can peruse the up-to-the-minute
result of its invocation in my blog scanner on the <a href="/blog/kw/">keywords page for this blog</a>.  Perl:
</p><p>
<pre>
sub keyword_tagger {
   my $ct = shift @_;
 
   my $weight;
   my $font;
   my $sm = 70;
   my $lg = 200;
   my $del = $lg - $sm;
   my $ret = '';
   foreach my $k (sort keys %kw_count) {
      $weight = $kw_count{$k} / $max_count;
      $font = sprintf ("%d", $sm + $del * $weight);
      $ret .= "&lt;a href=\"/blog/kw/$k/\" style=\"font-size: $font%;\"&gt;$k&lt;/a&gt;\n";
   }
 
   return $ret;
}
</pre>
</p><p>
This is generally <i>not</i> the way to structure a function call, because it works with global hashes, but y'know, I don't follow rules
too well (and curse myself often, yes).  The assumptions:
</p><p>
<ul>
<li>The only argument passed is the maximum post count for all tags, determined by an earlier scan of the tags while writing their index pages.</li>
<li><code>$sm</code> and <code>$lg</code> are effectively configuration; they determine the smallest and largest font sizes of the tag links (in percent).</li>
<li>The loop runs through the tags in alphabetical order; they are all assumed to be in the <code>%kw_count</code> global hash, which stores the number of posts associated with each tag (we build that while scanning the posts).</li>
<li>For every tag, we look at its post count in the <code>%kw_count</code> hash and split the difference in percentages between <code>$sm</code> and <code>$lg</code> -- then format the link with that font size.  Obviously, this is a rather overly hardwired approach (the link should obviously be a configurable template) but as a prototype and for my own blogging management script, this works well.</li>
</ul>
</p><p>
For our file cloud builder, we'll want to do this very same thing, but in Python (since that's our target language).  But porting is cake, now that we
know what we'll be porting.
</p><p>
Thus concludes the sermon for today.
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="a_quick_note_on_project_structure.html">A quick note on project structure</a></div>
<div class="st2">2006-12-02 <a href="keyword_programming.html">programming</a> <a href="keyword_projects.html">projects</a> <a href="keyword_tagger.html">tagger</a></div><br>
There are two general ways to approach software design; each has its uses.
</p><p>
<i>Top-down</i> design looks at the entire project and breaks it into
high-level components; those components are then subprojects and can be
further handled in the same way.
</p><p>
<i>Bottom-up</i> design looks at the resources available and sees likely
things that can be done with them; the idea is to provide generalized
components to be used in any project.
</p><p>
A healthy software design ecology has a lot of bottom-up components at
varying stages of maturity; those components then inform the top-down
requirements of the current project, giving those designs something to
work with.  In the absense of complex components, we're forced to write
everything from scratch, and it all turns into ad-hockery of the worst
kind.
</p><p>
Anyway, that item of philosophy out of the way, I wanted to talk about
the design of this week's project, the drop tagger.  There are three
main components of the drop tagger, as follows:
</p><p>
<ul>
<li><b>The drop handler</b><br>The drop handler is the component which
interacts with the shell and provides something you can drop files
onto or otherwise tag them.  It calls the file manager.  However, the
notion of a general drop handler is a much more interesting one than
a special-purpose drop handler just for this project, and one which can
be a valuable addition to many different file-oriented projects.
<li><b>The file manager</b><br>The file manager shows us what files have
been dropped, allows us to add and delete them and modify their tags,
and for fresh drops it will actively ask for tags.  It also calls the
tag cloud formatter and provides a convenient place to display the cloud.
<li><b>The cloud formatter</b><br>This is likely to be the least general
and thus the least interesting of these components, but it formats the
file cloud upon request based on information compiled about the tags in
the system.
</ul>
</p><p>
Each of these components can be designed and used in isolation, and reused
in other projects.  Alternatively, once we've defined the components we
need to meet our goal, we may well be able to find ready-made components
already available (or at least something we can adapt instead of starting
from scratch).  There is then a maturity effect over the course of multiple
projects, as our codebase allows us to be faster and faster responding to
the need for a project.
</p><p>
I'd like to formalize this design process over the course of several
mini-projects.  Stay tuned for further progress.
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="aquaponics_perl_programming_and_the_systems_approach.html">Aquaponics, Perl programming, and the systems approach</a></div>
<div class="st2">2006-12-01 <a href="keyword_aquaponics.html">aquaponics</a> <a href="keyword_systems-approach.html">systems-approach</a> <a href="keyword_programming.html">programming</a></div><br>
Did I mention that I'm not only going to be covering technical topics on this blog?  Today's word, kids, is "Aquaponics".
</p><p>
<i>Aquaculture</i> is growing fish for food.  <i>Hydroponics</i> is growing food (or other) plants in water or other non-soil rooting
medium.  <i>Aquaponics</i> is using the fish water as the hydroponic nutrient solution, which does two things for you: the plants filter
the nutrients (ammonia is fish urine but plant ambrosia) out of the water, so they don't choke the fish but instead are converted
into, say, lettuce; and the fish provide 
completely organic and relatively balanced set of nutrients for the plants.  So the combination is superior than either together, which
makes perfect sense if you consider that two smaller ecologies put together into one bigger one are necessarily more balanced and stable.
</p><p>
Anyway, that's our family's project this week.  We had already wanted to grow lettuce indoors for the winter, and so instead of
simply growing lettuce, we are growing lettuce <i>floating in a styrofoam block on an aquarium</i>.  The aquarium will have goldfish, so we
won't be eating that end of the system -- but it could just as well have tilapia in it.  In fact, tilapia are great aquaculture fish because
they'll essentially eat anything.  If they don't eat it, it just feeds algae, and they eat the algae instead.
</p><p>
So after we level up with 25 gallons of goldfish tank, I am very seriously considering building a much larger tank in the backyard under
a geodesic dome (per Organic Gardening of 1972) and growing me some serious tilapia.  Did you know that in a round pool 12 feet in diameter
and 3 feet deep (a small section of our backyard) you can harvest 500 half-pound tilapia every couple of months?  No, neither did I until today.
</p><p>
Anyway, I see all this as related to programming.  Both are simply the design of systems to meet needs.  And in fact, I find the way I think
about an aquaponics system is very similar to the way I think about a general data processing system.  Where an aquaponics system outputs
lettuce, a data processing system outputs some information I want.  To make lettuce, I need to consider the nutrients and water and light; to
make valuable information I need to consider the available raw data.
</p><p>
In either case, I find that a small, modular approach works well.  In the case of aquaculture, it's a matter of considering what nutrients
are where and what organisms can convert one thing to another; whereas in software, it's a matter of seeing simple data structures and designing
lightweight tools that can convert one to another -- and then you organize all your little modules/organisms into an ecology.
</p><p>
Lately, there have been two (software) projects I've worked
on in which this systems approach has worked well.  The new Toon-o-Matic is composed of a number of small, relatively simple Perl scripts which
are all organized by a Makefile.  Each script reads one or two or three input data structures, and emits one or two.  The overall network
could be drawn as a graph (and indeed, that would be edifying and entertaining, and I should do that.)
</p><p>
The other such system is this blog.  I've deliberately kept the approach simple and completely <i>sui generis</i>.  I'm reinventing the wheel
to a certain extent, but that's the attraction -- I like new wheels, and the occasional flaw doesn't bother me, as I always learn.  Evolution
doesn't mind reinventing the wheel -- did you know that the eye has evolved many completely separate times?  The eyes of insects, vertebrates, and
molluscs are three completely independent instances of the evolution of a visual sensor.  And the eyes of molluscs (like octopi) are demonstrably
superior to ours: our retinal nerves are in <i>front</i> of our retinae, thus each eye has a blind spot where the optic nerve penetrates the retina
to leave the eyeball.  Molluscs sensibly have their retinal nerves <i>behind</i> the retina: no blind spot.  Another reason to believe in Intelligent
Design -- just, you know, not of us.  God loves the octopus, which is why global warming is going to provide the octopus with lots of shallow,
warm seas with recently vacated cities in them.
</p><p>
Anyway, back on something resembling a track: my ultimate goal in the case of aquaculture is to close the ecological loop.  I want to take
my kitchen and garden waste, recycle it with
vermiculture and composting, feed the worms and plants to tilapia, use the fish water for lettuce and seedlings and the worm castings for root
vegetables, and ultimately I believe it may well be possible to feed my family fresh fish and veggies with not much more input than cardboad, grass
clippings, and leaves, and whatever's on sale at Kroger.
</p><p>
My goal in the case of most data processing systems is less lofty: I simply want to model some useful process in small, easily maintained and
easily modified steps, so that the system remains flexible and reliable.  But in either case, the thought processes are similar: to attain a large
goal, break it down into small, reusable task utilities.
</p><p>
I'll keep you posted on both.
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="complexification_programmatical_art.html">Complexification (programmatical art)</a></div>
<div class="st2">2006-11-30 <a href="keyword_programming.html">programming</a> <a href="keyword_art.html">art</a></div><br>
I got wind of this wonderful, wonderful artist because I was obsessing
on my hit logs due to starting this
little blog (ahem, not like <i>you</i> haven't done that, be honest),
and Xerexes at Comixpedia linked to this beauty of a site saying "Shades
of the <a href="/toonbots/">Toonbot</a>" (aww, I've become a Concept.  How cool.)
</p><p>
Anyway, my little scrivenings (hot dogs that they are) are nothing compared
to the absolute jawdropping sheer beauty at
<a href="http://www.complexification.net/">Gallery of Computation | generative artifacts</a>|<a href="http://www.complexification.net/">Gallery of Computation | generative artifacts</a>.  You gotta see it.
</p><p>
The site is the turf of one Jared Tarbell, whose modus operandi is to
write programs which express graphics.  Pretty graphics.  Really, really
pretty graphics.
</p><p>
Well -- enough bubbling.  Suffice it to say that I'd like to include a
scripting engine into some version of the Toon-o-Matic which allows this
kind of generative graphics.  I doubt I'll ever get it that pretty, but
still -- a man can dream.
</p><p>
Incidentally, note that this post's title contains parentheses.  I'm probably
revealing myself to be a complete fool, but let's just say that my blog
weaving code choked on it because I was doing something really stupid with
regular expressions.  That may very well be the subject of a post soon.
Or maybe I should keep my more egregious bugs under my hat.  No, wait, those
metaphors mix rather uncomfortably...
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="microsoft_word_macros_for_boxed_text.html">Microsoft Word macros for boxed text</a></div>
<div class="st2">2006-11-28 <a href="keyword_programming.html">programming</a> <a href="keyword_msword.html">msword</a> <a href="keyword_word-macro.html">word-macro</a> <a href="keyword_translation.html">translation</a></div><br>
One of the neat little things I did over the past few days was a
simple Word macro -- at least, it <i>should</i> have been simple, but
the problem is one I've had for a long time.
</p><p>
In this case, what I wanted to do was to fix up a few documents I had from
a translation customer.  This particular end user, for reasons known only
to them, captions their figures using fields.  The fields are in text boxes
for easy positioning, and the field results (the text you see on the screen)
are the captions.
</p><p>
Only one problem: the fields are always variable results for <i>variables
which don't exist in the document</i>.  All I can figure is that the
document preparer makes these things in little snippets with some other tool
which spits out Word texts, then they paste those into the text boxes.
</p><p>
So, you're asking now (unless you're a professional translator) who cares?
You just type your English over the German in the captions, and you're
home free, right?  Well: no.  Everybody who's anybody in the wonderful
world of translation nowadays uses translation tools, in this case TRADOS.
</p><p>
TRADOS does two things for you: it stores each and every sentence you translate
in a <i>translation memory</i> (a TM), so you (sort of) never need to 
translate anything twice, and it also makes it much easier to step through
a document translating.  The use of TRADOS makes translation <i>much</i>
easier, and it also helps you stay consistent in your use of words and
phrases.
</p><p>
Herein lies the problem: those fields were untouchable by TRADOS.  There are
two modes in TRADOS: one steps through the document using Word macros
but doesn't deal well
with text boxes (and yes, you'll note they're in
text boxes).  So that approach was out.
The other (the TagEditor) converts the entire document to an
XML format, then edits that in a very convenient way.  The TagEditor makes
short work of text boxes, but those field results were invisible to it.
</p><p>
Stuck!  And so for a series of three jobs from that customer, I just didn't
use TRADOS on the figure attachments, and hated it.  Last week, though, I
took screwdriver in hand (metaphorically speaking) and decided it was
showdown time.
</p><p>
OK, that's the teaser -- follow the link to get the ... <i>rest</i> of the
story.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="welcome_to_all_and_sundry.html">Welcome to all and sundry</a></div>
<div class="st2">2006-11-27 <a href="keyword_blogmeta.html">blogmeta</a></div><br>
Welcome to the new blog.  This is an idea I've been kicking around since
the day I heard the term "web log" (we hadn't abbreviated it), but I just
haven't had the time until now.  You know how it is.
</p><p>
The idea of the blog is simple.  I've been programming for a very long
time now, and I like to write about it.  So when I discovered that after a
significant hiatus from daily programming, my Muse had reawakened, I resolved
that from this point on, I would spent at least a small amount of time each
day programming <i>something</i>.  The results have been gratifying; I've
completed a number of small tasks I'd been wanting to resolve for a while.
</p><p>
And hence this blog.  This is the place where I intend to present said
small tasks, on a daily basis, for your perusal and, dare I hope, your
amusement.  I hope you enjoy it; I know I'm going to.
</p><p>
Some of the things I want to cover in the near future are, in no particular
order,
<ul>
<li>A few Word macros I've written to fix things up after TRADOS macros break them
<li>My experiences getting Python to read and write TTX files (a file used by TRADOS -- I earn my money with translation nowadays, mostly, so that's why translation tools come up so often
<li>This very blog, which is a freaking Rube Goldberg contraption of Perl scripts and Makefiles
<li>The amazing Toon-o-Matic, which I use to spin out graphics for my <a href="/toonbots/">Web cartoon</a> -- remember, the Toon-o-Matic is the work of art; the strip is just a by-product, like hot dogs
<li>GUI builders for wxPython, something I've been hacking around on for a while without discernable progress (but I hope that will be changing)
<li>Workflow applications, <a href="/wftk.html">of course</a>.
</ul>
</p><p>
That ought to keep me in posting fodder for a few months, eh?
</p><p>
</p><p>
</p><p>
</p><p>
</p><p>
<br>
</td>
</tr>
</table>

</div></td></td></table>

<br><br><br><br>
    <center><img src="/images/black.gif" height=1 width=300><br>
    <Font Size="-1"><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</Font>
</center>


</body>
</html>
