<html>
<head>
<title>Keyword nlp</title>
<style>p, li, td {
font-family: arial, sans-serif;
}
p.smtitle {
margin-left:0px;background-color:#eeeeee;font-weight:bold;
}
.sidemenu p {
font-size:small;
margin-top:0px;
margin-bottom:2px;
margin-left:10px;
}
.title {
font-family: arial, sans-serif;
font-weight: bold;
font-size:x-large;
color:black;
background-color:#eeeeee;
}
.subtitle {
font-family: arial, sans-serif;
font-size:small;
}
.t2 {
font-family: arial, sans-serif;
font-weight: bold;
font-size:large;
color:black;
background-color:#eeeeee;
}
.st2 {
font-family: arial, sans-serif;
font-size:x-small;
}
.border {
border: 1px solid #336600;
}
.content {
}
.code {
  margin: .5em 1em;
  padding: 0.5em;
  border: 1px dashed #94bd8d;
  color: Black;
  background-color: #eff7ef;
  overflow: auto;
}
.synComment    { color: #0000FF }
.synConstant   { color: #FF00FF }
.synIdentifier { color: #008B8B }
.synStatement  { color: #A52A2A ; font-weight: bold }
.synPreProc    { color: #A020F0 }
.synType       { color: #2E8B57 ; font-weight: bold }
.synSpecial    { color: #6A5ACD }
.synUnderlined { color: #000000 ; text-decoration: underline }
.synError      { color: #FFFFFF ; background: #FF0000 none }
.synTodo       { color: #0000FF ; background: #FFFF00 none }
.linenum       { color: #222222 ; background: #EEEEEE none }
</style>
</head>
<body>
<table width="100%">
<tr>
<td valign="top" width="150px"><div class="sidebar">
<div class="border">
<div class="sidemenu">
<p class="smtitle">vivtek</p>
<p>[ <a href=/>home</a> ]</p>
<p>[ <a href=/blog/>blog</a> ]</p>
<p>[ <a href=/recent.html>recent</a> ]</p>
<p>[ <a href=/projects/>programming</a> ]</p>
<p>[ <a href=/translation/>translation</a> ]</p>
<p>[ <a href=/fiction/>fiction</a> ]</p>
<p>[ <a href=/contact.html>contact</a> ]</p>
</div>

<div class="sidemenu">
<p class="smtitle">blog</p>
<p>[ <a href="keywords.html">keywords</a> ]</p>
<p class="smtitle">blogger</p>
<p>[ <a href="http://semantic-programming.blogspot.com/">semprog</a> ]</p>
<p>[ <a href="http://startup-ideas.blogspot.com/">startups</a> ]</p>
<p>[ <a href="http://orgaprop.blogspot.hu/">politics</a> ]</p>
</div>

<hr />
<script type="text/javascript"><!--
google_ad_client = "pub-7508846022405297";
google_ad_width = 120;
google_ad_height = 600;
google_ad_format = "120x600_as";
google_ad_type = "text";
google_ad_channel = "";
//--></script>
<script type="text/javascript"
  src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>

</div>

</div>
</td>
<td valign="top"><div class="content">

<div class="title">Keyword nlp</div>
<div class="st2"></div>
<hr>
<table>
<tr>
<td><div class="title"><a href="chatbot_framework.html">Chatbot framework</a></div>
<div class="st2">2007-10-07 <a href="keyword_chatbot.html">chatbot</a> <a href="keyword_nlp.html">nlp</a></div><br>
Something I've wanted to do for a couple of years now is a sort of online
chatbot framework thing.  In other words, this would be a testbed for different
language analysis techniques that could be played with online and tested
against real people.
</p><p>
An extension would be to connect a given chatbot to some other chatbot out
there somewhere, and see them talk to each other.  That could be fun.
</p><p>
The basic framework for this kind of venture could be pretty simple, but
could, of course, end up arbitrarily complex.  You'd need some kind of 
principled semantic framework (which would start at a simple box with words
in it and ramify through increasingly sophisticated syntactic and semantic
analyses -- the idea is to have a framework which can support both simplistic
single-word pattern matching to select a response, or use sentence frames
to extract some subject patterns to be manipulated in the response, right
up through a hypothetical Turing-complete NLP parser.)
</p><p>
The session would contain a list of facts and "stuff" which corresponds to the,
I dunno, dialog memory of a conversation.  There could optionally be some kind
of database memory of earlier conversations with a given contact.  Again, this
would run the gamut from simple named strings to be substituted into a response
pattern, to complete semantic structures of unknown nature, which would be
used to generate more sophisticated conversation.
</p><p>
Then the third and final component would be the definition of a chatbot itself.
This would consist of a set of responses to given situations (a situation being the current string incoming plus whatever semantic structure has been
accumulated during the course of the conversation.)  There could be a
spontaneity "response", i.e. something new said after some period of time
without an answer from the other.  Again -- it should be possible to start
small and stupid, with simple word patterns, random-response lists, and the
like, and build upwards to more complicated semantics.
</p><p>
The ability to detect and switch languages would be of great use, of course,
and there should be some kind of facility for that as well.
</p><p>
Wouldn't it be nice to be able to build a chatbot for language practice in,
say, Klingon or L&aacute;adan?  I mean, how else could you reasonably practice
a constructed language?
</p><p>
Anyway, when I have time, I'll certainly be doing something with this idea.  Any year now, yessir, any year.
</p><p>
</p><p>
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="perl_and_nlp.html">Perl and NLP</a></div>
<div class="st2">2014-05-20 <a href="keyword_nlp.html">nlp</a></div><br>
So I've long had a thing about doing NLP-type stuff in Perl.  I know, I know. All the cool kids
use NLTK in Python. So why Perl?
</p><p>
As always, the answer is CPAN. I can get a good, quick start in nearly anything by installing a CPAN
module, and I know it has been tested on Windows already thanks to CPANtesters. And anything <i>I</i>
write will be tested six ways from Sunday, too.
</p><p>
So Perl.
</p><p>
A few years ago I hacked out the beginnings of a tokenizer for NLP usage. It really just consisted of
a convenient iterator wrapper around some very simple regexes, along with some n-gram type stuff for
collocations (not that I've ever had much luck with those - yet). I've revived it and I've been
tossing some actual translation jobs at it to see what sticks, and it's nearly ready for release.
</p><p>
I had the revelation, though, that what even NLTK is missing in terms of <i>practical</i> use is
that it's a mess trying to retrieve information from documents. So my tokenizer explicitly works with
a source document, which can deliver a series of text and formatting commands in a pre-tokenization step.
The formatting commands are passed right through by the tokenizer.
</p><p>
Along the way, I realized that to do part-of-speech tagging I was going to need a lexicon. I've got a
dumb model of a lexicon running against SQLite (which will be good for job-specific vocabulary), but for
the main lexicon in German, it just isn't possible to get around the morphological structure of the
German language. So I'm currently adapting the <a href="http://www.j3e.de/ispell/igerman98/">igerman98 ispell dictionary</a>.
Its affix script is a pretty good run-down of German morphology, although it doesn't encode parts of
speech very accurately. (Nouns are capitalized, of course, and adjectives/adverbs are pretty much "A"-flagged
decliners.)
</p><p>
There's going to be a lot of tweaking involved, but the end result is going to be a pretty good
data-based lexicon that can probably fall back on some educated guesses for parts of speech of unknown
words.
</p><p>
Here's the kicker. If the part of speech is ambiguous at the word level, Marpa can simply figure it out
from context (usually). I think I have a good plan for this, but until I have a reasonable coverage
of parts of speech in my lexicon, I won't have anything to experiment with yet.
</p><p>
Soon, though, I'm going to be able to make some specific contributions to making NLP in Perl a reality.
I've been talking about doing this for a long, long time indeed. It's exciting to be actually making
progress with it for a change.
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="text_nsp.html">Text::NSP</a></div>
<div class="st2">2014-05-21 <a href="keyword_nlp.html">nlp</a></div><br>
Oh, now this is cool. My lexicon has also been a convenient place to collect n-grams, but I've never found the raw n-grams to be all that helpful. Turns out (as is so often the case) <i>I've been doing it wrong</i>. There are a boatload of statistical measures of n-grams that try to capture how much more frequently a collocation appears than would be expected by sheer chance given the probability of the individual words.
</p><p>
The entire boatload appears to be encoded in <a href="https://metacpan.org/pod/Text::NSP">Text::NSP</a>, which from a cursory examination of its chief command <tt>count.pl</tt> appears to be readily adapted to my tokenizer. That's entirely cool. I'm looking forward to getting set up to the point that I can get serious about my 10-million-word corpus of technical German. (Yeah, that's how much I've translated in my career so far, starting in about 2004 and continuing to today.)
</p><p>
Soon, compadre. Soon!
<br>
</td>
</tr>
<tr>
<td><div class="title"><a href="test_driven_lexicography.html">Test-driven lexicography</a></div>
<div class="st2">2014-05-24 <a href="keyword_nlp.html">nlp</a></div><br>
I'm getting hip-deep in German lexicography these days; since I can actually realistically tokenize
my translation documents now, I keep trying to, well, tokenize my translation documents now.
</p><p>
I've broken things up into Lingua::Lex, which is general tools for management of a lexicon, and Lingua::Lex::DE,
which is specifically my German lexicon. This allows me to test things at the level of the mechanism in one
place, and at the level of specific lexical rules in another. (Note: I haven't gotten as far as testing
specific lexical rules: but it's there in potential, anyway.)
</p><p>
The distribution version of a Lex::DE is flat files with convenient ASCII-only encoding; as part of the
setup procedure we build an SQLite database for the actual lexical work. I'm not sure how big the actual
lexicon will end up being yet, but from the igerman98 distro, it's not frighteningly large or impossible
for CPAN to handle. We'll see.
</p><p>
The lexical rules from igerman98 only scratch the surface - and they're grammatically naive, as there was
no reason to try to encode parts of speech, so for instance any word that takes an 's' on the end can be
lumped into the same flag no matter why it takes that 's'. Is it genitive? Plural? Something else? A spell
checker doesn't care, but a lexicon driving a parser does.
</p><p>
So there's work to be done, and I have the framework hammered out. I've made a lot of progress with it;
the compounding mechanism works, and suffixes are mostly working as of today. Once I start trying to tokenize
real text, then improvements should proceed apace. I figure maybe another week before I'm to the point of
trying to feed these token streams to Marpa - but that's not too long at all!
<br>
</td>
</tr>
</table>

</div></td></td></table>

<br><br><br><br>
    <center><img src="/images/black.gif" height=1 width=300><br>
    <Font Size="-1"><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</Font>
</center>


</body>
</html>
